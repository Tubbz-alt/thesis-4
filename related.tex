\chapter{Related Work}

%
%
%
\Section{related-openie}{Open Information Extraction}
% Talk about OpenIE
There is a large body of work on open information extraction.
One line of work begins with
  TextRunner \cite{key:2007yates-textrunner} and
  ReVerb \cite{key:2011fader-reverb}, which make use of computationally
  efficient surface patterns over tokens.
With the introduction of fast dependency parsers,
  Ollie \cite{key:2012mausam-ollie} continues in the same spirit but with
  learned dependency patterns, improving on the earlier WOE system
  \cite{key:2010wu-openie}.
The Never Ending Language Learning project \cite{key:2010carlson-nell}
  has a similar aim, iteratively learning more facts from the internet
  from a seed set of examples.
Exemplar \cite{key:2013mesquita-exemplar} adapts the open IE framework to
  $n$-ary relationships similar to semantic role labeling, but without the
  expensive machinery.

% Uses of Open IE
Open IE triples have been used in a number of applications --
  for example, learning entailment graphs for new triples
  \cite{key:2011berant-entailment}, and
  matrix factorization for unifying open IE and structured relations
  \cite{key:2012yao-schemas,key:2013riedel-schemas}.
In each of these cases, the concise extractions provided by open IE allow
  for efficient symbolic methods for entailment, such as Markov logic
  networks or matrix factorization.

% -- Extending OpenIE
% (people extending OpenIE)
A natural alternative to the approach taken in this paper is to
  extend knowledge bases by inferring and adding new facts directly.
% (misc)
For instance,
  \newcite{key:2006snow-wordnet} present an approach to enriching 
    the WordNet taxonomy;
  \newcite{key:2011tandon-conceptnet} extend ConceptNet with new facts;
  \newcite{key:2010soderland-adapting} use ReVerb extractions to 
    enrich a domain-specific ontology.
% Richard
\newcite{key:2013chen-completion} and \newcite{key:2013socher-completion}
  use Neural Tensor Networks to predict unseen relation triples in
  WordNet and Freebase, following a line of work by
  \newcite{key:2011bordes-completion} and
  \newcite{key:2012jenatton-completion}.
% Universal schemas
\newcite{key:2012yao-schemas} and \newcite{key:2013riedel-schemas}
  present a related line of work, inferring new relations between
  Freebase entities via inference over both Freebase and
  OpenIE relations.
In contrast, this work runs inference over arbitrary text, without 
  restricting itself to a particular set of relations, or even entities.
%This work, however, focuses primarily on common-sense reasoning rather
%  than inferring relations between named entities.


%
%
%
\Section{related-kbp}{Knowledge Base Population}


% Talk about relation extraction
Prior work on the KBP challenge can be categorized into a number of approaches.
The most common of these are \textit{distantly supervised} relation extractors
  \cite{key:1999craven-distsup,key:2007wu-distsup,key:2009mintz-distsup,key:2011sun-kbp},
  and rule based systems
  \cite{key:1997soderland-kbp,key:2010grishman-kbp,key:2010chen-kbp}.
However, both of these approaches require careful tuning to the task, and
  need to be trained explicitly on the KBP relation schema.
\newcite{key:2013soderland-kbp} submitted a system to KBP making use of
  open IE relations and an easily constructed mapping to KBP relations;
  we use this as a baseline for our empirical evaluation.

% -- Applications
Many NLP applications query large knowledge bases.
Prominent examples include
  question answering
    \cite{key:2001voorhees-trec},
  semantic parsing
    \cite{key:1996zelle-semantics,key:2007zettlemoyer-semantics,key:2013kwiatkowski-semantics,key:2014berant-semantics},
  and information extraction systems
    \cite{key:2011hoffman-kbp,key:2012surdeanu-mimlre}.
A goal of this work is to improve accuracy on these
  downstream tasks by providing a \textit{probabilistic} knowledge base
  for
%  both explicitly known and 
  likely true facts.


%
%
%
\Section{related-commonsense}{Common Sense Reasoning}

% -- GOFAI
% (intro)
The goal of tackling common-sense reasoning is by no means novel in
  itself.
Work by Reiter and McCarthy \cite{key:1980reiter-logic,key:1980mccarthy-circumscription}
  attempts to reason about the truth of a consequent
  in the absence of strict logical entailment.
Similarly, \newcite{key:1989pearl-probabilistic} presents a framework for
  assigning confidences to inferences which can be reasonably assumed.
Our approach differs from these attempts in part in its use of Natural Logic
  as the underlying inference engine, and more substantially in its
  attempt at creating a broad-coverage system.
More recently, work by \newcite{key:2002schubert-commonsense} and
  \newcite{key:2009durme-commonsense} approach common sense reasoning
  with \textit{episodic logic}; we differ in our focus on inferring
  truth from an arbitrary query, and in making use of longer inferences.
%  dealing with millions of candidate antecedents.



%
%
%
\Section{related-rte}{Textual Entailment}

% -- RTE
This work is similar in many ways to work on 
  recognizing textual entailment -- e.g., 
  \newcite{key:2010-schoenmackers-horn}, \newcite{key:2011berant-entailment}.
Work by \newcite{key:2013lewis-entailment} is particularly relevant,
  as they likewise evaluate on the FraCaS suite (Section 1;
  89\% accuracy with gold trees).
They approach entailment by constructing a CCG parse of the query,
  while mapping questions which are paraphrases of each other to the
  same logical form using distributional relation clustering.
However, their system is unlikely to scale to either our large
  database of premises, or our breadth of relations.

% Natural Logic
Prior work has used natural logic
  for RTE-style textual entailment,
  as a formalism well-suited for formal semantics in neural networks,
  and as a framework for common-sense reasoning
  \cite{key:2009maccartney-natlog,key:2012watanabe-natlog,key:2014bowman-natlog,key:2014angeli-naturalli}.
We adopt the precise semantics of \newcite{key:2014icard-natlog}.
Our approach of finding short entailments from a longer utterance is similar
  in spirit to work on textual entailment for information extraction
  \cite{key:2006romano-ie}.

% -- RTE
This work is similar in many ways to work on 
  recognizing textual entailment -- e.g., 
  \newcite{key:2010-schoenmackers-horn}, \newcite{key:2011berant-entailment},
  \newcite{key:2013lewis-entailment}.
In the RTE task, a single premise and a single hypothesis are given as input,
  and a system must return a judgment of either \textit{entailment} or
  \textit{nonentailment} (in later years, \textit{nonentailment} is further
  split into contradiction and independence).
These approaches often rely on alignment features, similar to ours, but
  do not generally scale to large premise sets (i.e., a comprehensive
  knowledge base).
The discourse commitments in \newcite{key:2007hickl-rte} can be thought
  of as similar to the additional entailed facts we add to the
  knowledge base (\refsec{naturalli-forward}).



%
%
%
\Section{related-rte}{Question Answering}

% Paraphrase-based Q/A
\newcite{key:2014fader-openqa} propose a system for question answering
  based on a sequence of paraphrase rewrites followed by a fuzzy query to
  a structured knowledge base.
This work can be thought of as an elegant framework for unifying this
  two-stage process, while explicitly tracking the ``risk'' taken with
  each paraphrase step.
Furthermore, our system is able to explore mutations which are only
  valid in one direction, rather than the bidirectional entailment of
  paraphrases, and does not require a corpus of such paraphrases for
  training.


% -- Q/A
Many systems make use of structured knowledge bases for question
  answering.
Semantic parsing methods 
  \cite{key:2005zettlemoyer-semantics,key:2011liang-semantics}
  use knowledge bases like Freebase to find support for a
  complex question.
% Richard
Knowledge base completion 
  (e.g., \newcite{key:2013chen-completion}, \newcite{key:2011bordes-completion},
  or \newcite{key:2013riedel-schemas}) can be thought of as entailment,
  predicting novel knowledge base entries from the original database.
%  can be like
%  use Neural Tensor Networks to predict unseen relation triples in
%  WordNet and Freebase, following a line of work by
%  \newcite{key:2011bordes-completion} and
%  \newcite{key:2012jenatton-completion}.
%% Universal schemas
%\newcite{key:2012yao-schemas} and \newcite{key:2013riedel-schemas}
%  present a related line of work, inferring new relations between
%  Freebase entities via inference over both Freebase and
%  OpenIE relations.
In contrast, this work runs inference over arbitrary text without
  needing a structured knowledge base.
% Paraphrase-based Q/A
Open IE \cite{key:2010wu-openie,key:2012mausam-ollie}
  QA approaches -- e.g., \newcite{key:2014fader-openqa}
  are closer to operating over plain text, but
  still requires structured extractions.
%\newcite{key:2014fader-openqa} perform question answering by following
%  fuzzy rewrites over a question to find support in a knowledge base
%  of open IE extractions.

% -- Logic-based QA
The COGEX system \cite{key:2003moldovan-trec} incorporates a theorem
  prover into a QA system, boosting overall performance.
Similarly, Watson \cite{key:2010ferrucci-watson} incorporates
  logical reasoning components.
This work follows a similar vein, but both the theorem prover
  and lexical classifier operate over text, without requiring either
  the premises or axioms to be in logical forms.
  

% -- Aristo
% Dialog
On the Aristo corpus we evaluate on, \newcite{key:2015hixon-aristo} proposes
  a dialog system to augment a knowledge graph used for answering the questions.
This is in a sense an oracle measure, where a human is consulted while answering
  the question; although, they show that their additional extractions help
  answer questions other than the one the dialog was collected for.














