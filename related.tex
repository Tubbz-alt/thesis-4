\chapter{Related Work}

%
% KBP + RE
%
\Section{related-kbp}{Knowledge Base Population}

Knowledge base population is the task of taking a large body of unstructured text,
  and extracting from it a structured knowledge base.
Importantly, the knowledge base has a fixed schema of relations (e.g.,
  \ww{born in}, \ww{spouse of}), usually with associated type signatures.
These knowledge bases can then be used as a repository of knowledge for
  downstream applications -- albeit restricted to the schema of the knowledge base itself.
In fact, many downstream NLP applications do query large knowledge bases.
Prominent examples include
  question answering systems
    \cite{key:2001voorhees-trec},
  and semantic parsers
    \cite{key:1996zelle-semantics,key:2007zettlemoyer-semantics,key:2013kwiatkowski-semantics,key:2014berant-semantics}.

% Talk about relation extraction
Prior work in this area can be categorized into a number of approaches.
The most common of these are \textit{supervised} relation extractors
  \cite{key:2004doddington-ace,key:2005zhou-ace,key:2007surdeanu-ace},
  \textit{distantly supervised} relation extractors
  \cite{key:1999craven-distsup,key:2007wu-distsup,key:2009mintz-distsup,key:2011sun-kbp},
  and rule based systems
  \cite{key:1997soderland-kbp,key:2010grishman-kbp,key:2010chen-kbp}.


% this is a classification problem
Relation extraction can be naturally cast as a supervised classification problem.
A corpus of relation mentions is collected,
  and each mention $x$ is annotated
  with the relation $y$, if any, it expresses. The classifier's output
  is then aggregated to decide the relations between the two entities.

% costly to annotate
However, annotating supervised training data is generally
  expensive to perform at large scale.
Although resources such as Freebase or the TAC KBP knowledge base
  have on the order of millions of training tuples over
  entities it is not feasible to manually annotate the 
  corresponding mentions in the text.
This has led to the rise of \textit{distantly supervised} methods, which
  make use of this indirect supervision, but do not
  necessitate mention-level supervision.


\Fig{img/relation-extraction-setup}{0.5}{re-intro}{
  The relation extraction setup.
  For a pair of entities, we collect sentences which
    mention both entities.
  These sentences are then used to predict one or more relations between
    those entities.
  For instance, the  sentences containing both \textit{Barack Obama}
    and \textit{Hawaii} should support the \rel{state of birth} and
    \rel{state of residence} relation.
}

Traditional distant supervision makes the assumption that
  for every triple $(e_1, y, e_2)$ in a knowledge base between
  a subject $e_1$, a relation $y$, and an object $e_2$, every sentence
  containing mentions for $e_1$ and $e_2$ express the relation $y$.
% Example
For instance, taking \reffig{re-intro}, we would create a datum for each
  of the three sentences containing \ent{Barack Obama} and \ent{Hawaii}
  labeled with \rel{state of birth}, and likewise with
  \rel{state of residence}, creating 6 training examples overall.
Similarly, both sentences involving \textit{Barack Obama} and
  \textit{president} would be marked as expressing the \rel{title}
  relation.

% Downsides
While this allows us to leverage a large database effectively, it
  nonetheless makes a number of na\"{\i}ve assumptions.
First -- explicit in the formulation of the approach -- it assumes that every
  mention expresses some relation,
  and furthermore expresses the known relation(s).
For instance, the sentence \w{Obama visited Hawaii} would be erroneously
  treated as a positive example of the \rel{born in} relation.
Second, it implicitly assumes that our knowledge base is complete:
  entity mentions with no known relation are treated as negative examples.

% How these downsides are addressed
The first of these assumptions is addressed by
  multi-instance multi-label (MIML) learning, which puts an intermediate latent variable
  for each sentence-level prediction, that then has to predict the correct knowledge base
  triples \cite{key:2012surdeanu-mimlre}.
\newcite{key:2013min-incomplete} address the second assumption by extending the
  MIML model with additional latent variables, while \newcite{key:2013xu-filling}
  allow feedback from a coarse relation extractor to augment labels from the knowledge base.
These latter two approaches are compatible with but are 
  not implemented in this work.



% -- KBC
Lastly, there are approaches to inferring new facts in a knowledge base that do not
  make use of text at all.
% Richard
\newcite{key:2013chen-completion} and \newcite{key:2013socher-completion}
  use Neural Tensor Networks to predict unseen relation triples in
  WordNet and Freebase, following a line of work by
  \newcite{key:2011bordes-completion} and
  \newcite{key:2012jenatton-completion}.
% Universal schemas
\newcite{key:2012yao-schemas} and \newcite{key:2013riedel-schemas}
  present a related line of work, inferring new relations between
  Freebase entities via inference over both Freebase and
  OpenIE relations.
In contrast, this work runs inference over arbitrary text, without 
  restricting itself to a particular set of relations, or even entities.



%
% OPEN IE
%
\Section{related-openie}{Open Information Extraction}
% Talk about OpenIE
One approach to broad-domain knowledge extraction is \textit{open information extraction} (Open IE).
Traditional relation extraction settings usually specify a domain of relations they're
  interested in (e.g., place of birth, spouse, etc.), and usually place restrictions on the
  types of arguments extracted (e.g., only people are born places).
Open IE systems generalize this setting to the case where both the relation and the arguments
  are represented as plain text, and therefore can be entirely open-domain.
For example, in the sentence \ww{the president spoke to the Senate on Monday}, we might extract
  the following triples:

\begin{displayquote}
  (\ww{president}; \ww{spoke to}; \ww{Senate}) \\
  (\ww{president}; \ww{spoke on}; \ww{Monday})
\end{displayquote}

Open information extraction (open IE) has been shown to be useful in a number of NLP tasks, such
  as question answering \cite{key:2014fader-openqa} relation 
  extraction \cite{key:2013soderland-kbp},
  and information retrieval \cite{key:2011etzioni-nature}. 
Open IE triples have been also been used in,
  for example, learning entailment graphs for new triples
  \cite{key:2011berant-entailment}, and
  matrix factorization for unifying open IE and structured relations
  \cite{key:2012yao-schemas,key:2013riedel-schemas}.
Open-domain triples have also been used to improve knowledge base population,
  described in \refsec{related-kbp}.
\newcite{key:2013soderland-kbp} submitted a system to KBP making use of
  open IE relations (see below) and an easily constructed mapping to KBP relations.
\newcite{key:2010soderland-adapting} use ReVerb extractions to 
    enrich a domain-specific ontology.
In each of these cases, the concise extractions provided by open IE allow
  for efficient symbolic methods for entailment, such as Markov logic
  networks or matrix factorization.


One line of work in this area are the early UW OpenIE systems: for example,
  TextRunner \cite{key:2007yates-textrunner} and
  ReVerb \cite{key:2011fader-reverb}.
In both of these cases, an emphasis is placed on \textit{speed} --
  token-based surface patterns are extracted that would correspond to
  open domain triples.
With the introduction of fast dependency parsers,
  \cite{key:2010wu-openie} extracts triples from
  learned dependency patterns.
Building upon this, Ollie \cite{key:2012mausam-ollie} also learns patterns from dependency
  parses, but with the additional contributions of (1) allowing for extractions which are
  mediated by nouns or adjectives, not just verbs; and (2) considering context more carefully
  when extracting these triples.
Exemplar \cite{key:2013mesquita-exemplar} adapts the open IE framework to
  $n$-ary relationships similar to semantic role labeling, but without the
  expensive machinery.


In another line of work, The Never Ending Language Learning project (NELL) \cite{key:2010carlson-nell}
  iteratively learns more facts from the internet
  from a seed set of examples.
In the case of NELL, the ontology is open-domain but fixed, and the goal becomes to learn
  all entries in the domain.
For example, learning an extended hypernymy tree (\ww{Post University} is a University);
  but also more general relations (\ww{has aqcuired}, \ww{publication writes about}, etc).


%
% COMMON SENSE
%
\Section{related-commonsense}{Common Sense Reasoning}

% -- GOFAI
% (intro)
The goal of tackling common-sense reasoning is by no means novel in
  itself either.
Work by Reiter and McCarthy \cite{key:1980reiter-logic,key:1980mccarthy-circumscription}
  attempts to reason about the truth of a consequent
  in the absence of strict logical entailment.
Similarly, \newcite{key:1989pearl-probabilistic} presents a framework for
  assigning confidences to inferences which can be reasonably assumed.
Our approach differs from these attempts in part in its use of Natural Logic
  as the underlying inference engine, and more substantially in its
  attempt at creating a broad-coverage system.
More recently, work by \newcite{key:2002schubert-commonsense} and
  \newcite{key:2009durme-commonsense} approach common sense reasoning
  with \textit{episodic logic}; we differ in our focus on inferring
  truth from an arbitrary query, and in making use of longer inferences.
%  dealing with millions of candidate antecedents.



%
% RTE
%
\Section{related-rte}{Textual Entailment}

% -- RTE
This thesis is in many ways to work on 
  recognizing textual entailment -- e.g., 
  \newcite{key:2010-schoenmackers-horn}, \newcite{key:2011berant-entailment}.
Textual Entailment is the task of determining if a given premise sentence
  entails a given hypothesis.
That is, if without additional context, a human would infer that the hypothesis
  is true if the premise is true.
For instance:

\begin{displayquote}
  \ww{I drove up to San Francisco yesterday} \\
  \ww{I was in a car yesterday}
\end{displayquote}

Although the definition of entailment is always a bit fuzzy -- what if I drove a train
  up to SF, or pehaps a boat? -- nonetheless a reasonable person would assume that if
  you drove somewhere you were in a car.
This sort of reasoning is similar to the goal of this thesis: given premises, infer
  valid hypothesis to claim as true.
However, in RTE the premise set tends to be very small (1 or 2 premises), and the domain
  tends to have less of a focus on common-sense or broad domain facts.


For example, work by \newcite{key:2013lewis-entailment}
  approach entailment by constructing a CCG parse of the query,
  while mapping questions which are paraphrases of each other to the
  same logical form using distributional relation clustering.
% Natural Logic
Prior work has used natural logic
  for RTE-style textual entailment,
  as a formalism well-suited for formal semantics in neural networks,
  and as a framework for common-sense reasoning
  \cite{key:2009maccartney-natlog,key:2012watanabe-natlog,key:2014bowman-natlog,key:2014angeli-naturalli}.
We adopt the precise semantics of \newcite{key:2014icard-natlog}.
Our approach of finding short entailments from a longer utterance is similar
  in spirit to work on textual entailment for information extraction
  \cite{key:2006romano-ie}.

%% -- RTE
%This work is similar in many ways to work on 
%  recognizing textual entailment -- e.g., 
%  \newcite{key:2010-schoenmackers-horn}, \newcite{key:2011berant-entailment},
%  \newcite{key:2013lewis-entailment}.
%In the RTE task, a single premise and a single hypothesis are given as input,
%  and a system must return a judgment of either \textit{entailment} or
%  \textit{nonentailment} (in later years, \textit{nonentailment} is further
%  split into contradiction and independence).
%These approaches often rely on alignment features, similar to ours, but
%  do not generally scale to large premise sets (i.e., a comprehensive
%  knowledge base).
%The discourse commitments in \newcite{key:2007hickl-rte} can be thought
%  of as similar to the additional entailed facts we add to the
%  knowledge base (\refsec{naturalli-forward}).



%
% QA
%
\Section{related-rte}{Question Answering}
% Paraphrase-based Q/A
\newcite{key:2014fader-openqa} propose a system for question answering
  based on a sequence of paraphrase rewrites followed by a fuzzy query to
  a structured knowledge base.
This work can be thought of as an elegant framework for unifying this
  two-stage process, while explicitly tracking the ``risk'' taken with
  each paraphrase step.
Furthermore, our system is able to explore mutations which are only
  valid in one direction, rather than the bidirectional entailment of
  paraphrases, and does not require a corpus of such paraphrases for
  training.


% -- Q/A
Many systems make use of structured knowledge bases for question
  answering.
Semantic parsing methods 
  \cite{key:2005zettlemoyer-semantics,key:2011liang-semantics}
  use knowledge bases like Freebase to find support for a
  complex question.
% Richard
Knowledge base completion 
  (e.g., \newcite{key:2013chen-completion}, \newcite{key:2011bordes-completion},
  or \newcite{key:2013riedel-schemas}) can be thought of as entailment,
  predicting novel knowledge base entries from the original database.
%  can be like
%  use Neural Tensor Networks to predict unseen relation triples in
%  WordNet and Freebase, following a line of work by
%  \newcite{key:2011bordes-completion} and
%  \newcite{key:2012jenatton-completion}.
%% Universal schemas
%\newcite{key:2012yao-schemas} and \newcite{key:2013riedel-schemas}
%  present a related line of work, inferring new relations between
%  Freebase entities via inference over both Freebase and
%  OpenIE relations.
In contrast, this work runs inference over arbitrary text without
  needing a structured knowledge base.
% Paraphrase-based Q/A
Open IE \cite{key:2010wu-openie,key:2012mausam-ollie}
  QA approaches -- e.g., \newcite{key:2014fader-openqa}
  are closer to operating over plain text, but
  still requires structured extractions.
%\newcite{key:2014fader-openqa} perform question answering by following
%  fuzzy rewrites over a question to find support in a knowledge base
%  of open IE extractions.

% -- Logic-based QA
The COGEX system \cite{key:2003moldovan-trec} incorporates a theorem
  prover into a QA system, boosting overall performance.
Similarly, Watson \cite{key:2010ferrucci-watson} incorporates
  logical reasoning components.
This work follows a similar vein, but both the theorem prover
  and lexical classifier operate over text, without requiring either
  the premises or axioms to be in logical forms.
  

% -- Aristo
% Dialog
On the Aristo corpus we evaluate on in \refchp{science}, 
  \newcite{key:2015hixon-aristo} proposes
  a dialog system to augment a knowledge graph used for answering the questions.
This is in a sense an oracle measure, where a human is consulted while answering
  the question; although, they show that their additional extractions help
  answer questions other than the one the dialog was collected for.














