\Section{results}{Experiments}
We evaluate our system on two tasks: the FraCaS test suite, used
  by MacCartney and Manning 
  \cite{key:2007maccartney-natlog,key:2008maccartney-natlog}
  evaluates the system's ability to capture Natural Logic inferences
  even without the explicit alignments of these previous systems.
In addition, we evaluate the system's ability to predict common-sense
  facts from a large corpus of OpenIE extractions.
%  as well as for filtering valid and invalid extractions from the
%  ReVerb OpenIE system \cite{key:2011fader-reverb}, as per
%  \newcite{key:2013angeli-truth}.

%
% FraCaS
%
\Subsection{results-fracas}{FraCaS Entailment Corpus}
The FraCaS corpus \cite{key:1996cooper-fracas}
  is a small corpus of entailment problems, aimed at providing a
  comprehensive test of a system's handling of various
  entailment patterns.
We process the corpus following \newcite{key:2007maccartney-natlog}.
%12 problems were discarded as degenerate -- lacking either an antecedent or
%  a consequent.
%151 problems were discarded as involving multiple antecedents to
%  justify the inference.
It should be noted that many of the sections of the corpus
  are not directly applicable to Natural Logic inferences;
  \newcite{key:2007maccartney-natlog} identify three sections which
  are in the scope of their system, and consequently our system as well.

\def\a#1{\textbf{#1}}
\begin{table}
\begin{center}
  \begin{tabular}{|l|l|c|cc|cc|ccc|}
    \hline
    $\mathsection$ & Category & Count & \multicolumn{2}{c|}{Precision} & \multicolumn{2}{c|}{Recall} & \multicolumn{3}{c|}{Accuracy} \\
                   &          &       & \blue{N} & M08          & \blue{N} & M08          & \blue{N}  & M07 & M08 \\
    \hline
                          % Pme           P07   Rme          R08   Ame         A07  A08
    \a{1} & \a{Quantifiers}  & \a{44} & \a{\blue{91 }}  & \a{95}  & \a{\blue{100}} & \a{100} & \a{\blue{95}} & \a{84} & \a{97} \\
    2     & Plurals          & 24     & \blue{80 }      & 90      & \blue{29 }     & 64      & \blue{38}     & 42     & 75 \\
    3     & Anaphora         & 6      & \blue{100}      & 100     & \blue{20 }     & 60      & \blue{33}     & 50     & 50 \\
    4     & Ellipses         & 25     & \blue{100}      & 100     & \blue{5  }     & 5       & \blue{28}     & 28     & 24 \\
    \a{5} & \a{Adjectives}   & \a{15} & \a{\blue{80 }}  & \a{71}  & \a{\blue{66 }} & \a{83}  & \a{\blue{73}} & \a{60} & \a{80} \\
    \a{6} & \a{Comparatives} & \a{16} & \a{\blue{90 }}  & \a{88}  & \a{\blue{100}} & \a{89}  & \a{\blue{87}} & \a{69} & \a{81} \\
    7     & Temporal         & 36     & \blue{75 }      & 86      & \blue{53 }     & 71      & \blue{52}     & 61     & 58 \\
    8     & Verbs            & 8      & \blue{$-$}      & 80      & \blue{0  }     & 66      & \blue{25}     & 63     & 62 \\
    9     & Attitudes        & 9      & \blue{$-$}      & 100     & \blue{0  }     & 83      & \blue{22}     & 55     & 89 \\
    \hline
\multicolumn{2}{|l|}{\a{Applicable (1,5,6)}}
                         & \a{75}     & \a{\blue{89}}   & \a{89}  & \a{\blue{94}}  & \a{94}  & \a{\blue{89}} & \a{76} & \a{90} \\
    \hline
  \end{tabular}
  \longcaption{NaturalLI's accuracy on the FraCaS textual entailment suite.}
  {
    Results on the FraCaS textual entailment suite.
    N is this work;
    M07 refers to \newcite{key:2007maccartney-natlog};
      M08 refers to \newcite{key:2008maccartney-natlog}.
    The relevant sections of the corpus intended to be handled by this
      system are sections 1, 5, and 6 (although not 2 and 9, which are also
      included in M08).
    \label{tab:fracas}
  }
\end{center}
\end{table}

Results on the dataset are given in \reftab{fracas}.
Since the corpus is not a blind test set, the results are presented
  less as a comparison of performance, but rather to validate
  the expressive power of our search-based approach against
  MacCartney's align-and-classify approach.
For the experiments, costs were set to express valid
  Natural Logic inference as a hard constraint.
%  logical entailment system -- costs corresponding to valid Natural
%  Logic mutations were set to a small constant cost; other costs
%  were set to infinity.

The results show that the system is able to capture Natural Logic
  inferences with similar accuracy to the state-of-the-art system of
  \newcite{key:2008maccartney-natlog}.
Note that our system is comparatively crippled in this framework
  along at least two dimensions:
It cannot appeal to the premise when constructing the search,
  leading to the introduction of a class of search errors which 
  are entirely absent from prior work.
Second, the derivation process itself does not have access to the
  full parse tree of the candidate fact.
%Therefore, it is unable to make large edits over constituents,
%  and is unable to appeal to the parse in its mutation decisions.
%For instance, our system has no notion of prepositional attachment.

Although precision is fairly high even on the non-applicable
  sections of FraCaS, recall is significantly lower than prior work.
This is a direct consequence of not having alignments to appeal to.
For instance, we can consider two inferences:

\vspace{-0.25em}
\begin{itemize}
\item[] \w{Jack saw Jill is playing} $\xRightarrow{?}$ \w{Jill is playing}
\item[] \w{Jill saw Jack is playing} $\xRightarrow{?}$ \w{Jill is playing}
\end{itemize}
\vspace{-0.25em}

It is clear from the parse of the sentence that the first is valid
  and the second is not; however, from the perspective of the search
  algorithm both make the same two edits: inserting \w{Jack} and \w{saw}.
In order to err on the side of safety, we disallow deleting the verb
  \w{saw}.

%For instance, differentiating between \w{Smith saw} \w{Smith thinks that}
%  in \w{Smith saw/thinks that Jones sign the contract} is very difficult
%  without having access to the full phrase being 
%  an antecedent is a conjunction of the consequent and another fact.
%However, it is nearly impossible for a search from the consequent to
%  justify halucinating the second fact in the antecedent.
%Similarly, in Section 9 (Attitudes), it is easy for an alignment to
%  recognize phrases like \w{He said that $\dots$} 
%
%Despite this, the system performs comparable to NatLog on the sections
%  it intends to capture.
%Furthermore, precision is consistently high among all sections, although
%  recall is usually significantly lower.
%It should also be noted that \newcite{key:2008maccartney-natlog} 
%  intend to capture the inferences in sections 2 (plurals)
%  and 9 (attitudes), which we omit from our system.

%
% ConceptNet
%
\Subsection{results-conceptnet}{Common Sense Reasoning}

\begin{table}
\begin{center}
  \begin{tabular}{l|cc:c|c}
    System             & P     & R    & F$_1$  & Accuracy \\
    \hline
    Lookup             & 100.0 & 12.1 & 21.6 & 56.0 \\
    NaturalLI Only     & 88.8  & 40.1 & 55.2 & 67.5 \\
    NaturalLI + Lookup & 90.6  & 49.1 & 63.7 & 72.0  \\
  \end{tabular}
  \longcaption{NaturalLI's accuracy inferring common-sense facts from ConceptNet.}
  {
    Accuracy inferring common-sense facts on a balanced test set.
    \textit{Lookup} queries the lemmatized lower-case fact directly in the
      270M fact database.
    \textit{NaturalLI Only} disallows such lookups, and infers every query from
      only distinct premises in the database.
    \textit{NaturalLI + Lookup} takes the union of the two systems.
    \label{tab:conceptnet}
  }
\end{center}
\end{table}

We validate our system's ability to infer unseen common sense facts
  from a large database of such facts.
Whereas evaluation on FraCaS shows that our search formulation captures
  applicable inferences as well as prior work, this evaluation presents
  a novel use-case for Natural Logic inference.
%  which is not trivial given prior work.

For our database of facts, we run the Ollie OpenIE system
  \cite{key:2012mausam-ollie} over Wikipedia,\footnote{
    \url{http://wikipedia.org/} (2013-07-03)
  }
  Simple Wikipedia,\footnote{
    \url{http://simple.wikipedia.org/} (2014-03-25)
  }
  and a random \num{5}\% of CommonCrawl.
Extractions with confidence below 0.25 or which contained
  pronouns were discarded.
This yielded a total of 305 million unique extractions composed
  entirely of lexical items which mapped into our vocabulary
  (\num{186707} items).
Each of these extracted triples $(e_1,r,e_2)$ was then flattened into
  a plain-text fact $e_1~r~e_2$ and lemmatized.
This yields \num{270} million unique lemmatized premises in our database.
In general, each fact in the database could be arbitrary unstructured
  text; our use of Ollie extractions is motivated only by a desire to
  extract short, concise facts.

For our evaluation, we infer the top 689 most confident facts from
  the ConceptNet project \cite{key:2011tandon-conceptnet}.
To avoid redundancy with WordNet, we take facts from eight
  ConceptNet relations: 
    MemberOf,
    HasA,
    UsedFor,
    CapableOf,
    Causes,
    HasProperty,
    Desires, and 
    CreatedBy.
We then treat the \textit{surface text} field of these facts as our
  candidate query.
This yields queries like the following:

\vspace{-0.25em}
\begin{itemize}
\item[] \w{not all birds can fly}
\item[] \w{noses are used to smell}
\item[] \w{nobody wants to die}
\item[] \w{music is used for pleasure}
\end{itemize}
\vspace{-0.25em}

For negative examples, we take the 689 ReVerb extractions
  \cite{key:2011fader-reverb}
  judged as false by Mechanical Turk workers \cite{key:2013angeli-truth}.
This provides a set of plausible but nonetheless incorrect examples,
%  similar in many ways to the database of Ollie extractions, 
  and ensures that our recall is not due to over-zealous search.
Search costs are tuned from an additional set of 540 true ConceptNet and
  540 false ReVerb extractions.

Results are shown in \reftab{conceptnet}.
We compare against the baseline of looking up each fact verbatim in the
  fact database.
Note that both the query and the facts in the database are 
  short snippets, already lemmatized and lower-cased;
  therefore, it is not in principle unreasonable to
  expect a database of 270 million extractions to contain these facts.
Nonetheless, only 12\% of facts were found via a direct lookup.
We show that NaturalLI (allowing lookups) improves this recall
  four-fold, at only an 9.4\% drop in precision.
%Furthermore, if we prohibit direct lookup,
%  we still recover all but 9\% of our recall.



%To evaluate our system in a more real-world setting, we adopt the
%  task of predicting the truth of ReVerb extractions
%  as introduced in \newcite{key:2013angeli-truth}.
%Equivalently, this can be viewed as filtering out semantically 
%  implausible extractions.
%
%For this data, 4000 randomly selected ReVerb extractions were shown to
%  Mechanical Turk workers, who ranked each extraction as either
%  \textit{correct}, \textit{plausible}, or \textit{implausible}.
%Extractions which obtained more \textit{correct} votes than
%  \textit{implausible} votes were considered true, and visa versa.
%Ties were discarded from the corpus.
%The examples are broken up into 1796 training examples ($70\%$ true)
%  and 1975 test examples ($65\%$ true).
%
%Results are given in \reffig{mturk}.
%They're not very good yet.
%Hopefully they'll get better.
