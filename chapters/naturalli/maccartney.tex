\Section{maccartney}{MacCartney's Natural Logic}
% Motivate Natural Logic
Natural Logic aims to capture common logical
  inferences by appealing directly to the structure of language,
  as opposed to running deduction on an abstract logical form.
The logic builds upon traditional rather than first-order logic:
  to a first approximation, Natural Logic can be seen as
  an enhanced version of Aristotle's syllogistic system
  \cite{key:2008vanbenthem-natlog}.
A working understanding of the logic as syllogistic reasoning
  is sufficient for understanding the later contributions of the paper.
%Natural Logic is inspired largely by the Aristotelian
%  syllogisms and subsequent traditional logics
%  \cite{key:2008vanbenthem-natlog}.
While some inferences of first-order logic are not captured by 
  Natural Logic,
  it nonetheless allows for a wide range of intuitive inferences in
  a computationally efficient and conceptually clean way.
%That is to say, Natural Logic takes the meaning representation of a
%  sentence to be the surface form of the sentence itself.
%In part, this lends itself to computationally efficient inference;
%  in another part, it frees the system from parsing to an abstract
%  logic -- this is particularly relevant in our case, where the set
%  of potential antecedents is very large.

% Introduce MacCartney + Related Work
We build upon the variant of the logic introduced by
  the NatLog system (MacCartney and Manning, 2007; 2008; 2009),
  \nocite{key:2007maccartney-natlog}
  \nocite{key:2008maccartney-natlog}
  \nocite{key:2009maccartney-natlog}
  based on earlier theoretical work on Natural Logic and 
  Monotonicity Calculus
  \cite{key:1986benthem-natlog,key:1991valencia-natlog}.
Later work formalizes many aspects of the logic
  \cite{key:2012icard-natlog,key:2013djalali-natlog};
  we adopt the formal semantics of
  \newcite{key:2014icard-natlog}, along with much of their
  notation.
%  which we appeal to but are not necessary for a 
%  appeal to in future sections, but are not necessary for an understanding
%  of the contributions of this work.
%An elegant introduction to Natural Logic can be found in
%  \newcite{key:2014icard-natlog}; a thorough treatment of
%  MacCartney's Natural Logic can be found in \newcite{key:2009maccartney-natlog}.

At a high level, Natural Logic proofs operate by mutating spans of text
  to ensure that the mutated sentence follows from the
  original -- each step is much like a syllogistic inference.
We construct a complete proof system in three parts:
  we define MacCartney's atomic relations between lexical entries
  (\refsec{maccartney-relations}), the effect these lexical mutations have
  on the validity of the sentence (\refsec{maccartney-polarity}),
  and a practical approach for executing these proofs.
We review MacCartney's alignment-based approach in
  \refsec{maccartney-proof}, and show that we can generalize and
  simplify this system in \refsec{search}.

%
% MacCartney Relations
%
\Subsection{maccartney-relations}{Lexical Relations}

% -- Relations Figure --
\begin{figure}[t]
\begin{center}
    \begin{tabular}{ccc}
      \equivalentVenn & \forwardVenn & \reverseVenn \\
      \negateVenn & \alternateVenn & \coverVenn
    \end{tabular}
\end{center}
\caption{
  The model-theoretic interpretation of the MacCartney relations.
  The figure shows the relation between the denotation of
    $\varphi$ (dark) and $\psi$ (light).
  The universe is denoted by $\sD$.
  \label{fig:relations}
}
\end{figure}
% --                 --

\newcite{key:2007maccartney-natlog} introduce seven set-theoretic
  relations between the denotations of any two lexical items.
The denotation of a lexical item is the set of objects in the domain
  of discourse $\sD$ to which that lexical item refers.
For instance, the denotation of \w{cat} would be the set of all cats.
Two denotations can then be compared in terms of
  set algebra: if we define the set of cats to be $\varphi$ and
  the set of animals to be $\psi$, we can state that
  $\varphi \subseteq \psi$.
%This example should evoke a taste of Natural Logic: e.g., if
%  $\varphi \subseteq \psi$, then anything which holds for all elements
%  in $\psi$ (all animals) must hold for all elements of $\varphi$
%  (all cats).

The six informative relations are summarized in \reffig{relations};
  a seventh relation (\independent) corresponds to to the completely
  uninformative relation.
For instance, the example search path in \reffig{teaser} makes use of
  the following relations:

\vspace{1.0em}
\begin{center}
\begin{tabular}{rcl}
\w{No $x$ $y$} & \negate     & \w{The $x$ $y$} \\
\w{cat}        & \forward    & \w{carnivore} \\
\w{animal}     & \equivalent & \w{a animal} \\
\w{animal}     & \reverse    & \w{mouse} \\
\end{tabular}
\end{center}
\vspace{1.0em}

%The middle two middle entries have straightforward interpretations:
%  the set of cats is a subset of the set of carnivores; the set of
%  mice is a subset of animals.
%The last entry states that the denotation of \w{mouse} and
%  \w{a mouse} is close enough that they can be considered equivalent.

Denotations are not required to be in the space of predicates
  (e.g., \textit{cat}, \textit{animal}).
In the first example, the denotations of \textit{No} and \textit{The}
  are in the space of operators
  \mbox{$p \rightarrow (p \rightarrow t)$}:
  functions from predicates $p$ to truth values $t$.
%The first entry is somewhat subtle in that the denotation of quantifiers
%  is not over the universe of entities $e$, but rather over functions
%  from entities to truth values $t$:
The \negate\ relation becomes the conjunction of two claims:
  $\forall x \forall y ~ \lnot\left(\w{no}~x~y \land \w{the}~x~y \right)$
  and
  $\forall x \forall y ~ \left(\w{no}~x~y \lor \w{the}~x~y\right)$.
This is analogous to the construction of the
  set-theoretic definition of \negate\ in \reffig{relations}:
  $\varphi \cap \psi = \varnothing$ and $\varphi \cup \psi = \sD$
  (see \newcite{key:2014icard-natlog}).
%A more thorough treatment can be found in
%  \newcite{key:2014icard-natlog}.

%Note that \forward\ and \reverse\ are inverses.
Examples of the last two relations (\alternate\ and \cover)
and the complete independence relation (\independent) include:

\begin{center}
\begin{tabular}{rcl}
\w{cat}        & \alternate   & \w{dog} \\
\w{animal}     & \cover       & \w{nonhuman} \\
\w{cat}        & \independent & \w{friendly} \\
\end{tabular}
\end{center}

%We proceed to describe the relationship between these lexical mutations,
%  and the validity of executing the mutation in a derivation.

%
% Monotonicity & Polarity
%
\Subsection{maccartney-polarity}{Monotonicity and Polarity}
% Motivate
The previous section details the relation between 
  lexical items;
  however, we still need a theory for how to ``project'' the relation
  induced by a lexical mutation as a relation between the two
  containing sentences.
%  ``up the parse tree'' to become a relation between two sentences.
For example, \textit{cat} \forward\ \textit{animal},
  and \textit{some cat meows} \forward\ \textit{some animal meows},
  but
  \textit{no cat barks} \nforward\ \textit{no animal barks}.
Despite differing by the same lexical relation,
  the first example describes a valid entailment, 
  while the second does not.

% Introduce Monotonicity
We appeal to two important concepts: \textit{monotonicity} as a
  property of arguments to natural language operators,
  and \textit{polarity}
  as a property of lexical items in a sentence.
Much like monotone functions in calculus,
  an [upwards] monotone operator has an output truth value which is
  non-decreasing (i.e., material implication)
  as the input ``increases'' (i.e., the subset relation).
From the example above, \textit{some} is upwards monotone in its first
  argument, and \textit{no} is downwards monotone in its first argument.

%% Explain monotone
%To offer a concrete example, \textit{some} is a monotone quantifier taking two
%  entities as input
%  (i.e., \w{some $x$ $y$} -- \w{some cats eat mice}),
%  and returning a truth value.
%Therefore, we can replace an argument to \w{some} with a superset of that
%  argument, and be guaranteed that the truth value of the new
%  predicate will be ``at least'' the truth value of the original statement.
%The ``at least'' relation on truth values is trivially defined
%  as $F \leq T$; therefore, for two truth values $t_1$ and $t_2$,
%  $t_1 \leq t_2$ is precisely material implication $t_1 \Rightarrow t_2$.
%With this we can now show that, e.g., 
%  \w{some cats eat mice} $\Rightarrow$ \w{some animals eat mice}.
%
%% Whirlwind through antitone + nonmonotone
%Analogous to monotone quantifiers, we can define \textit{antitone}\footnote{
%    Monotone and antitone are often referred to as
%    \textit{monotone up} and \textit{monotone down}.
%  }
%  quantifiers as quantifiers which have an output truth value which is
%  non-increasing as the input increases.
%For instance, \w{no} is an antitone quantifier.
%Note that quantifiers can have different monotonicity for each argument:
%  \w{all} is monotone in its first and antitone in its second argument.
%Furthermore, not all quantifiers are monotone or antitone --
%  \w{most} is the classic example of a quantifier which is
%  \textit{nonmonotone} in its first argument.

% Polarity
%So far we have looked only at sentences with a single quantifier;
%  moreover, monotonicity itself is not always sufficient to warrant
%  mutations on lexical items.
\textit{Polarity} is
  a property of lexical items in a sentence determined by the
  operators acting on it.
All lexical items have \textit{upward} polarity by default;
  upwards monotone operators preserve polarity,
  and downwards monotone operators reverse polarity.
For example, \w{mice} in \w{\textbf{no} cats eat mice} has downward 
  polarity, whereas \w{mice} in \w{\textbf{no} cats do\textbf{n't} eat mice}
  has upward polarity
  (it is in the scope of two downward monotone operators).
% Projection
The relation between two sentences differing by a single lexical
  relation is then given by the projection function $\rho$ in
  \reftab{projectivity}.\footnote{
    Note that this table optimistically assumes every operator is
    \textit{additive} and \textit{multiplicative}, as defined
    in \newcite{key:2012icard-natlog}.
  }

% -- Projection Table --
% I'm assuming the context is additive and multiplicative
% See Lemma 2.4 (p.11) of http://link.springer.com/article/10.1007%2Fs11225-012-9425-8
\begin{table}[t]
	\begin{center}
%	\begin{tabular}{c|cc}
%    \textbf{Relation} & \multicolumn{2}{c}{\textbf{Polarity of Context}} \\
%             & Upward & Downward \\
%    \hline
%     $p \forward p'    $ & $s \forward     s'$ & $s \reverse     s'$ \\ 
%     $p \reverse p'    $ & $s \reverse     s'$ & $s \forward     s'$ \\ 
%     $p \alternate p'  $ & $s \alternate   s'$ & $s \cover       s'$ \\ 
%     $p \cover p'      $ & $s \cover       s'$ & $s \alternate   s'$ \\ 
%     $p \negate p'     $ & $s \negate      s'$ & $s \negate      s'$ \\ 
%	\end{tabular}
  \begin{tabular}{r|ccccccc}
    $r$ & \equivalent & \forward & \reverse & \alternate & \cover & \negate & \independent \\
    \hline
    $\rho(r)$ & \equivalent & \reverse & \forward & \cover & \alternate & \negate & \independent \\
  \end{tabular}
	%(caption)
	\caption{
    The projection function $\rho$, shown for downward polarity  
      contexts only.
    The input $r$ is the lexical relation between two words in
      a sentence;
      the projected relation $\rho(r)$ is the relation between the
      two sentences differing only by that word.
    Note that $\rho$ is the identity function in upward polarity 
      contexts.
%       a lexical item $e_1$ and
%      its candidate mutation $e_2$, in terms of the projected relation
%      between sentence $s_1$ and the new sentence $s_2$.
%    Values are given for when the predicates are in an upwards and
%      downwards polarity.
%    Note that \equivalent\ and \independent\ project up unchanged.
		\label{tab:projectivity}
	}
	\end{center}
\end{table}


%We now have a repertoire or lexical mutations, and a polarity marking
%  on each lexical item.
%We have two remaining elements to define for a full proof theory:
%  how do lexical mutations of a word with a certain polarity
%  \textit{trickle up} the sentence to affect the relation between
%  entire sentences, and how do we chain individual lexical mutations
%  together for a full derivation.
%The first is studied in depth by \newcite{key:2012icard-natlog};\footnote{
%    Formally, we optimistically assume every quantifier 
%    is \textit{additive} and \textit{multiplicative}.
%  }
%  we axiomatically define the relation between two sentences differing
%  by a mutation in \reftab{projectivity}.
%The second is discussed in the next section.

%
% Inference as Alignment
%
\Subsection{maccartney-proof}{Proof By Alignment}

% -- Join Table--
\begin{table}[t]
	\begin{center}
	\begin{tabular}{|c||c|c|c|c|c|c|c|}
    \hline
    $\bowtie$ & $\equivalent$ & $\forward$ & $\reverse$ & $\negate$ & $\alternate$ & $\cover$ & $\independent$ \\
    \hline
    $\equivalent$ & $\equivalent$ & $\forward$ & $\reverse$ & $\negate$ & $\alternate$ & $\cover$ & $\independent$ \\
    $\forward$ & $\forward$ & $\forward$ & $\independent$ & $\alternate$ & $\alternate$ & $\independent$ & $\independent$ \\
    $\reverse$ & $\reverse$ & $\independent$ & $\reverse$ & $\cover$ & $\independent$ & $\cover$ & $\independent$  \\
    $\negate$ & $\negate$ & $\cover$ & $\alternate$ & $\equivalent$ & $\reverse$ & $\forward$ & $\independent$  \\
    $\alternate$ & $\alternate$ & $\independent$ & $\alternate$ & $\forward$ & $\independent$ & $\forward$ & $\independent$  \\
    $\cover$ & $\cover$ & $\cover$ & $\independent$ & $\reverse$ & $\reverse$ & $\independent$ & $\independent$  \\
    $\independent$ & $\independent$ & $\independent$ & $\independent$ & $\independent$ & $\independent$ & $\independent$ & $\independent$ \\
    \hline
	\end{tabular}
	%(caption)
	\caption{
    The join table as shown in \newcite{key:2012icard-natlog}.
    Entries in the table are the result of joining a row with a
      column.
%    Note that the $\independent$ always joins to yield $\independent$,
%    and $\equivalent$ always joins to yield the input relation.
		\label{tab:join}
	}
	\end{center}
\end{table}

\newcite{key:2007maccartney-natlog} approach the inference task in
  the context of inferring whether a single relevant premise entails
  a query.
%In the framework of \newcite{key:2007maccartney-natlog}, the task is
%  to infer whether a single logical antecedent entails a query
%  consequent.
Their approach first generates an alignment between the premise
  and the query, and then classifies each aligned segment into one of
  the lexical relations in \reffig{relations}.
Inference reduces to projecting each of these relations
  according to the projection function $\rho$ (\reftab{projectivity}) 
  and iteratively \textit{joining}  two projected relations together to 
  get the final entailment relation.
This join relation, denoted as $\bowtie$, is given in \reftab{join}.

To illustrate, we can consider MacCartney's example inference from
  \w{Stimpy is a cat} to \w{Stimpy is not a poodle}.
An alignment of the two statements would provide three lexical
  mutations:
    \mbox{$r_1\coloneqq\w{cat}\rightarrow\w{dog}$}, 
    \mbox{$r_2\coloneqq\cdot\rightarrow\w{not}$},
    and \mbox{$r_3\coloneqq\w{dog}\rightarrow\w{poodle}$}.
%The initial relation $r_0$ is axiomatically \equivalent.
Each of these are then projected with the projection function $\rho$,
  and are joined using the join relation:

\begin{equation*}
  r_0 \bowtie \rho(r_1) \bowtie \rho(r_2) \bowtie \rho(r_3),
\end{equation*}

\noindent where the initial relation $r_0$ is axiomatically \equivalent.
In MacCartney's work this style of proof is presented as a table.
The last column ($s_i$) is the relation between the premise and the
  $i^{th}$ step in the proof, and is constructed inductively as
  $s_i \coloneqq s_{i-1} \bowtie \rho(r_i)$:

\begin{center}
\begin{tabular}{rl|ccc}
  \multicolumn{2}{c|}{Mutation} & $r_i$ & $\rho(r_i)$ & $s_i$ \\
  \hline
  $r_1$ & \w{cat}$\rightarrow$\w{dog}    & \alternate & \alternate & \alternate \\
  $r_2$ & $\cdot\rightarrow$\w{not}      & \negate    & \negate    & \forward \\
  $r_3$ & \w{dog}$\rightarrow$\w{poodle} & \reverse   & \forward   & \forward \\
\end{tabular}
\end{center}

%\noindent where $x_i$ is the surface form of the fact at inference step
%  $i$, $\beta(e_i)$ is the lexical relation, as per
%  \reffig{relations}, $\beta(x_{i-1}, e_i)$ is the relation projected
%  according to \reftab{projectivity}, and $\beta(x_0, x_i)$ is
%  the relation between the antecedent and the current candidate fact.
%$\beta(x_0, x_i)$ is obtained from repeatedly applying the join table
%  in \reftab{join} to $\beta(x_{0}, x_{i-1})$ and $\beta(x_{i-1}, e_i)$.
%For instance, joining $\beta(x_1, x_1)$ with $\beta(x_1, e_2)$
%  (\alternate\ $\bowtie$ \negate) yields $\beta(x_1, x_2)$ (\forward).
%
%The final relation between the antecedent and the consequent is taken
%  to be the last $\beta(x_0, x_i)$ entry.

In our example, we would conclude that
  \w{Stimpy is a cat} \forward\ \w{Stimpy is not a poodle}
  since $s_3$ is \forward;
  therefore the inference is valid.
