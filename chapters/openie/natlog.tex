\Section{extraction}{Intra-Clause Open IE}

% Intro
%For this section, we assume that our sentences consist of a single clause --
%  that is, a single open IE relation to extract.
%\refsec{clause} will describe how to split a longer sentence into such clauses.
We now turn to the task of generating a maximally compact sentence which retains
  the core semantics of the original utterance, and parsing the sentence
  into a conventional open IE subject verb object triple.
This is often a key component in downstream applications, where extractions
  need to be not only \textit{correct}, but also \textit{informative}.
Whereas an argument like \ww{Heinz Fischer of Austria} is often correct,
  a downstream application must apply further processing to recover information
  about either \ww{Heinz Fischer}, or \ww{Austria}.
Moreover, it must do so without the ability to appeal to the larger context
  of the sentence.
%For example, \ww{friends give heartfelt praise} should entail that
%  \ww{friends give praise}, and consequently the triple \ww{(friends; give; praise)}.
%But, \ww{enemies give false praise} should not entail that
%  \ww{enemies give praise}.

\Subsection{natlog}{Validating Deletions with Natural Logic}
% Intro
We adopt a subset of natural logic semantics dictating
  contexts in which lexical items can be removed.
Natural logic as a formalism captures common logical inferences
  appealing directly to the form of language, rather than parsing to a
  specialized logical syntax.
It provides a proof theory for lexical mutations to a sentence which either
  preserve or negate the truth of the premise.

% Teaser on natural logic
For instance, if \ww{all rabbits eat vegetables} then 
  \ww{all cute rabbits eat vegetables}, since we are allowed to mutate
  the lexical item \ww{rabbit} to \ww{cute rabbit}.
This is done by observing that \ww{rabbit} is in scope of the first
  argument to the operator \ww{all}.
Since \ww{all} induces a \textit{downward polarity} environment for
  its first argument, we are allowed
  to replace \ww{rabbit} with an item which is more specific -- in this case
  \ww{cute rabbit}.
To contrast, the operator \ww{some} induces an \textit{upward polarity}
  environment for its first argument,
  and therefore we may derive the inference from
  \ww{cute rabbit} to \ww{rabbit} in:
  \ww{some cute rabbits are small} therefore \ww{some rabbits are small}.
% Wrap up teaser
For a more comprehensive introduction to natural logic, see
  \newcite{key:2008vanbenthem-natlog}.

%In this chapter, we make use of only the theory on when deletions in a sentence
%  are warranted.
We mark the scopes of all operators (\ww{all}, \ww{no}, \ww{many},
  etc.) in a sentence, and from this determine whether every lexical item
  can be replaced by something more general (has upward polarity),
  more specific (downward polarity), or neither.
In the absence of operators, all items have upwards polarity.

% Deleting edges as up/down the hierarchy
Each dependency arc is then classified into whether deleting the dependent
  of that arc makes the governing constituent at that node more general,
  more specific (a rare case), or neither.\footnote{
    We use the Stanford Dependencies representation 
      \cite{key:stanford-dependencies}.
  }
For example, removing the \typ{amod} edge in \hbox{\ww{cute $\xleftarrow{amod}$ rabbit}} yields the
  more general lexical item \ww{rabbit}.
However, removing the \typ{nsubj} edge in \hbox{\ww{Fido $\xleftarrow{nsubj}$ runs}} would yield the
  unentailed (and nonsensical) phrase \ww{runs}.
The last, rare, case is an edge that causes the resulting item to be 
  more specific -- e.g., \typ{quantmod}: 
  \hbox{\ww{about $\xleftarrow{quantmod}$ 200}} is more general than \ww{200}.

% This is approximate
For most dependencies, this semantics can be hard-coded with high accuracy.
However, there are at least two cases where more attention is warranted.
%Although for many dependency edges this is a good approximation,
%  there are at least two cases where more attention is warranted.
The first of these concerns non-subsective adjectives: for example 
  a \ww{fake gun} is not a gun.
For this case, we make use of the list of non-subsective adjectives collected
  in \newcite{key:2014nayak-adjectives}, and prohibit their deletion as a
  hard constraint.

% PP Attachment
The second concern is with prepositional attachment, and direct object
  edges.
For example, whereas \ww{Alice went to the playground $\xrightarrow{prep\_with}$ Bob} entails
  that \ww{Alice went to the playground}, it is not meaningful to infer that
  \ww{Alice is friends $\xrightarrow{prep\_with}$ Bob} entails \ww{Alice is friends}.
Analogously, \ww{Alice played $\xrightarrow{dobj}$ baseball on Sunday} 
  entails that \ww{Alice played
  on Sunday}; but, \ww{Obama signed $\xrightarrow{dobj}$ the bill on Sunday} should not entail the
  awkward phrase \ww{*Obama signed on Sunday}.

% Learning attachment affinity
We learn these attachment affinities empirically from the syntactic n-grams
  corpus of \newcite{key:2013goldberg-syngrams}.
This gives us counts for how often object and preposition edges occur in the
  context of the governing verb and relevant neighboring edges.
We hypothesize that edges which are frequently seen to co-occur 
  are likely
  to be essential to the meaning of the sentence.
To this end, we compute the probability of seeing an arc of a given type,
  conditioned on the most specific context we have statistics for.
These contexts, and the order we back off to more general contexts,
  is given in \reffig{affinity}.

% Backoff probabilities
\begin{figure}[t]
\begin{center}
  \begin{dependency}[text only label, label style={above}]
    \begin{deptext}[column sep=-0.1cm]
      Obama \& signed \& the \& bill \& into \& law \& on \& Friday \\
    \end{deptext}
    \depedge[edge unit distance=1.0ex]{2}{1}{nsubj}
    \depedge[edge unit distance=1.5ex]{2}{4}{dobj}
    \depedge[edge unit distance=1.0ex]{4}{3}{det}
    \depedge[edge unit distance=1.25ex]{2}{6}{prep\_into}
    \depedge[edge unit distance=1.20ex]{2}{8}{prep\_on}
  \end{dependency}

  \[ \verticalcenter{\rotatebox{90}{\textrm{prep backoff}}} \left\{ 
  \begin{array}{l}
    \textrm{\scriptsize $p\Big($prep\_on $\mid$
    \verticalcenter{\hspace{-0.5em}
    \begin{dependency}[text only label, label style={above}]
      \begin{deptext}[column sep=-0.0cm]
        Obama \& signed \& bill \\
      \end{deptext}
      \depedge[edge unit distance=1.0ex]{2}{1}{nsubj}
      \depedge[edge unit distance=0.5ex]{2}{3}{dobj}
    \end{dependency}
    \hspace{-0.5em}} $\Big)$} \\
    
    \textrm{\scriptsize $p\Big($prep\_on $\mid$
    \verticalcenter{\hspace{-0.5em}
    \begin{dependency}[text only label, label style={above}]
      \begin{deptext}[column sep=-0.0cm]
        Obama \& signed \& law \\
      \end{deptext}
      \depedge[edge unit distance=1.0ex]{2}{1}{nsubj}
      \depedge[edge unit distance=0.5ex]{2}{3}{prep\_into}
    \end{dependency}
    \hspace{-0.5em}} $\Big)$} \\

    \textrm{\scriptsize $p\Big($prep\_on $\mid$
    \verticalcenter{\hspace{-0.5em}
    \begin{dependency}[text only label, label style={above}]
      \begin{deptext}[column sep=-0.0cm]
        Obama \& signed \\
      \end{deptext}
      \depedge[edge unit distance=1.0ex]{2}{1}{nsubj}
    \end{dependency}
    \hspace{-0.5em}} $\Big)$} \\

    \textrm{\scriptsize $p\Big($prep\_on $\mid$ signed $\Big)$}
  \end{array} \right.\]

  \[ \verticalcenter{\rotatebox{90}{\textrm{dobj backoff}}} \left\{ 
  \begin{array}{l}
    \textrm{\scriptsize $p\Big($dobj $\mid$
    \verticalcenter{\hspace{-0.5em}
    \begin{dependency}[text only label, label style={above}]
      \begin{deptext}[column sep=-0.0cm]
        Obama \& signed \& bill \\
      \end{deptext}
      \depedge[edge unit distance=1.0ex]{2}{1}{nsubj}
      \depedge[edge unit distance=0.5ex]{2}{3}{dobj}
    \end{dependency}
    \hspace{-0.5em}} $\Big)$ } \\
    
    \textrm{\scriptsize $p\Big($dobj $\mid$ signed $\Big)$ }
  \end{array} \right.\]

\end{center}
\longcaption{Backoff order when deciding to drop
    a prepositional phrase or direct object for OpenIE extractions.}
{\label{fig:affinity}
  The ordered list of backoff probabilities when deciding to drop
    a prepositional phrase or direct object.
  The most specific context is chosen for which an empirical probability
    exists; if no context is found then we allow dropping prepositional
    phrases and disallow dropping direct objects.
  Note that this backoff arbitrarily orders contexts of the same size.
}
\end{figure}

% Backoff order
%\begin{enumerate}
%  \item The probability of seeing a PP edge, given the governing verb
%        and verb's direct object.
%  \item The probability of seeing a PP edge, given the governing verb
%        and a neighboring PP edge.
%  \item The probability of seeing a PP edge, given the governing verb.
%  \item The probability of seeing a particular object, given the governing
%        verb and a PP edge.
%  \item The probability of seeing any object, given the governing
%        verb and a PP edge.
%\end{enumerate}

% Inference time
To compute a score $s$ of \textit{deleting} the edge from the
  affinity probability $p$ collected from the syntactic n-grams, we simply
  cap the affinity and subtract it from 1:
%If the affinity probability from \reffig{affinity} is
%  $p$, the score $s$ of deleting the edge becomes:
\begin{equation*}
  s = 1 - \min(1, \frac{p}{K})
\end{equation*}
where $K$ is a hyperparameter denoting the minimum fraction of the time an
  edge should occur in a context to be considered entirely unremovable.
In our experiments, we set $K=\frac{1}{3}$.

% Total score
The score of an extraction, then, is the product of the scores of each
  deletion multiplied by the score from the clause splitting step
  in \refsec{clause}.


%
% Atomic Patterns
%
\Subsection{patterns}{Atomic Patterns}
% Verb patterns
\begin{table}[t]
\begin{center}
\begin{tabular}{l|l}
\textbf{Input} & \textbf{Extraction} \\
\hline
\ww{\small{cats play with yarn}}        & \small{(cats; play with; yarn)} \\
\ww{\small{fish like to swim}}          & \small{(fish; like to; swim)} \\
\ww{\small{cats have tails}}            & \small{(cats; have; tails)} \\
\ww{\small{cats are cute}}              & \small{(cats; are; cute)} \\
\ww{\small{Tom and Jerry are fighting}} & \small{(Tom; fighting; Jerry)} \\
\ww{\small{There are cats with tails}}  & \small{(cats; have; tails)}
\end{tabular}
\end{center}
\longcaption{The six dependency patterns used to segment an atomic sentence into an 
  open IE triple.}
{\label{tab:patterns}
  The six dependency patterns used to segment an atomic sentence into an 
  open IE triple.
}
\end{table}

% Nominal patterns
\begin{table}[t]
\begin{center}
\begin{tabular}{l|l}
\textbf{Input} & \textbf{Extraction} \\
\hline
\ww{\small{Durin, son of Thorin}}       & \small{(Durin; is son of; Thorin)} \\
\ww{\small{Thorin's son, Durin}}        & \small{(Thorin; 's son; Durin)} \\
\ww{\small{IBM CEO Rometty}}            & \small{(Rometty; is CEO of; IBM)} \\
\ww{\small{President Obama}}            & \small{(Obama; is; President)} \\
\ww{\small{Fischer of Austria}}         & \small{(Fischer; is of; Austria)} \\
\ww{\small{IBM's research group}}       & \small{(IBM; 's; research group)} \\
\ww{\small{US president Obama}}         & \small{(Obama; president of; US)} \\
\ww{\small{Our president, Obama,}}      & \small{(Our president; be; Obama)}
\end{tabular}
\end{center}
\longcaption{The eight patterns used to segment a noun phrase into an open IE
  triple.}
{\label{tab:nominal}
  The eight patterns used to segment a noun phrase into an open IE
  triple.
  The first five are dependency patterns; the last three are surface
    patterns.
}
\end{table}

% Verb patterns
Once a set of short entailed sentences is produced, it becomes straightforward
  to segment them into conventional open IE triples.
We employ 6 simple dependency patterns, given in \reftab{patterns},
  which cover the majority of
  atomic relations we are interested in.

% Noun patterns
When information is available to disambiguate the substructure of compound
  nouns (e.g., named entity segmentation),
  we extract additional relations with
  5 dependency and 3 TokensRegex \cite{key:stanford-tokensregex} surface
  form patterns.
These are given in \reftab{nominal}; we refer to these as \textit{nominal relations}.
Note that the constraint of named entity information is by no means
  required for the system.
In other applications -- for example, applications in vision -- 
  the otherwise trivial nominal relations could be quite useful.
  
%All six of these capture either of the declarative \ww{be} relation
%  (e.g., adjective modification) or simple variants of such
%  (e.g., \ww{Napoleon of France} entails \ww{Napoleon is of France}).
%  \todo{linguistics?}

%% Restrict NER types
%However, for all of these patterns we restrict both the subject and the object
%  to named entities.
%This (1) allows us to find sub-structure in noun phrases
%  (\ww{president Barack Obama} entails \ww{Barack Obama is president},
%  but not \ww{Obama is Barack}); and
%  (2) restricts the number of context specific extractions
%  (\ww{ferocious cats} does not entail that \ww{cats are ferocious} in a
%  general context).
%
%The second half of the triple extraction process is the segmenting of a
%  sentence into a subject verb object triple.
%We identify two types of relations commonly extracted by open IE systems:
%  those that center around a verb, and those that center around a noun.
%For example, \ww{cats have tails} versus \ww{Google CEO Larry Page}.
%
%% The verb patterns
%The first of these we capture with 8 simple hand-coded patterns.
%Sample extractions for each of the patterns are given in \reftab{patterns}.

