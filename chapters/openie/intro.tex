\Section{openie-intro}{Introduction}

\begin{figure}[t]
  \begin{center}
  \begin{tabular}{ll}
  \multicolumn{2}{c}{{\w{Born in Honolulu, Hawaii, Obama is a US Citizen.}}} \\
  \multicolumn{1}{c}{\scriptsize{\textbf{Our System}}} & \multicolumn{1}{c}{\scriptsize{\textbf{Ollie}}} \\
  {(Obama; is; US citizen)}             & {(Obama; is; a US citizen)} \\
  {(Obama; born in;}                    & {(Obama; be born in; Honolulu)} \\
  {$~~~~~$Honolulu, Hawaii)}            & {\darkred{(Honolulu; be born in; Hawaii)}} \\
                                                     & {(Obama; is citizen of; US)} \\
  \\ \\ 
  \multicolumn{2}{c}{{\w{Friends give true praise.}}} \\
  \multicolumn{2}{c}{{\w{Enemies give fake praise.}}} \\
  \multicolumn{1}{c}{\scriptsize{\textbf{Our System}}} & \multicolumn{1}{c}{\scriptsize{\textbf{Ollie}}} \\
  {(friends; give; true praise)}              & {(friends; give; true praise)} \\
  {(friends; give; praise)}                   & \\
  {(enemies; give; fake praise)}              & {(enemies; give; fake praise)} \\
  
  \\ \\ 
  \multicolumn{2}{c}{{\w{Heinz Fischer of Austria visits the US}}} \\
  \multicolumn{1}{c}{\scriptsize{\textbf{Our System}}} & \multicolumn{1}{c}{\scriptsize{\textbf{Ollie}}} \\
  {(Heinz Fischer; visits; US)}              & {(Heinz Fischer \darkred{of Austria};} \\
                                                          & {$~~~~~$ visits; \darkred{the} US)} \\
  \end{tabular}
  \longcaption{Open IE extractions produced by the system, alongside extractions from prior work.}
  {
    \label{fig:teaser}
    Open IE extractions produced by the system, alongside extractions from
      the state-of-the-art Ollie system.
    Generating coherent clauses before applying patterns helps reduce
      false matches such as \textit{(Honolulu; be born in; Hawaii)}.
    Inference over the sub-structure of arguments, in turn, allows us to
      drop unnecessary information (e.g., \ww{of Austria}), but only when
      it is warranted (e.g., keep \ww{fake} in \ww{fake praise}).
    }
  \end{center}
\end{figure}

% Segue
A key shortcoming of NaturalLI as presented in the previous chapter is its inability to reason
  over sufficiently long or syntactically complex premises.
In naturally occurring text, a sentence most often expresses a number of atomic facts -- NaturalLI
  is most efficient when each of its premises expresses only a single, isolated fact.
Fortunately, this is in itself a well-studied problem: open information extraction (open IE) -- creating open-domain triples from
  text -- can be thought of as extracting atomic propositions from a syntactically complex sentence.
In this chapter, we construct a system for open information extraction, which will also
  serve to segment a long sentence into atomic propositions suitable to be a premise set for NaturalLI.
We will revisit the merging of this open IE system into NaturalLI in the next chapter.


% Motivation
Open information extraction has been shown to be useful in a
  number of NLP tasks, such as question answering \cite{key:2014fader-openqa},
  relation extraction \cite{key:2010soderland-adapting}, 
  and information retrieval \cite{key:2011etzioni-nature}.
% Challenge: many rules
Conventionally, open IE systems search a collection of patterns over either
  the surface form or dependency tree of a sentence.
Although a small set of patterns covers most simple sentences 
  (e.g., subject verb object constructions),
  relevant relations are often spread across clauses (see
  \reffig{teaser}) or presented in a non-canonical form.

% Special case (Ollie)
Systems like Ollie \cite{key:2012mausam-ollie} approach this problem by 
  using a bootstrapping method to create a large corpus of broad-coverage
  partially lexicalized patterns.
Although this is effective at capturing many of these patterns,
  it can lead to unintuitive behavior on out-of-domain text.
%  this level of lexicalization can lead to unintuitive behavior on 
%  out-of-domain text.
For instance, while \ww{Obama is president} is extracted correctly by Ollie
  as \textit{(Obama; is; president)}, replacing \w{is} with \w{are} in 
  \ww{cats are felines} produces no extractions.
Furthermore, existing systems struggle at producing
  canonical argument forms -- for example, in \reffig{teaser} the argument
  \ww{Heinz Fischer of Austria} is likely less useful for downstream
  applications than \ww{Heinz Fischer}.
%  , either stripping relevant
%  information, or retaining irrelevant information in the argument.
%Both of these are harmful for downstream applications using the
%  output of the systems

% Sales pitch
In this work, we shift the burden of extracting informative and broad
  coverage triples away from this large pattern set.
Rather, we first pre-process the sentence in linguistically motivated ways
  to produce coherent clauses which are (1) logically entailed by the 
  original sentence, and (2) easy to segment into open IE triples.
% Overview of approach
Our approach consists of two stages:
  we first learn a classifier for splitting a sentence into shorter
  utterances (\refsec{clause}), 
  and then appeal to natural logic to maximally
  shorten these utterances while maintaining necessary context (\refsec{natlog}).
A small set of 14 hand-crafted patterns can then be used to segment an
  utterance into an open IE triple.

% Summarize clause splitting
We treat the first stage as a greedy search problem: we traverse
  a dependency parse tree recursively, at each step predicting whether an 
  edge should yield an independent clause.
%In the first stage, we traverse a tree recursively, allowing an edge
%  to optionally produce an independent clause.
Importantly, in many cases \naive ly yielding a clause on a dependency
  edge produces an incomplete utterance (e.g., \ww{Born in Honolulu, Hawaii},
  from \reffig{teaser}).
These cases are often attributable to control relationships, where either the
  subject or object of the governing clause controls the subject of the
  subordinate clause.
We therefore allow the produced clause to sometimes inherit 
  the subject or object of its governor.
%Therefore, we implicitly identify these sorts of constructions
%  and allow the controlled verb (e.g., \ww{born}) to inherit the subject 
%  of its controller (i.e., \ww{Obama}).
%Each split clause, in turn, can optionally borrow either the entire
%  governing clause, or just the subject of the governing clause.
%For example, the first sentence in \reffig{teaser} will produce the
%  clause \ww{Obama born in Honolulu, Hawaii}, borrowing the subject
%  \ww{Obama} from the governing clause.
This allows us to capture a large variety of long range 
  dependencies with a concise classifier.
%  while keeping both the state space of learned
%  classifier small and the set of atomic patterns small.
 
% Natural Logic
From these independent clauses, we then extract shorter sentences, which
  will produce shorter arguments more likely to be useful for downstream
  applications.
A natural framework for solving this problem is natural logic -- 
  the underlying formalism behind much of this dissertation (see \refchp{natlog}).
We can then observe that
  \ww{Heinz Fischer of Austria visits China} entails that
  \ww{Heinz Fischer visits China}.
%  but that \ww{enemies give fake compliments} does not entail that
%  \ww{enemies give compliments}.
On the other hand, 
%  it's often incorrect to maximally shorten an argument.
  we respect situations where it is incorrect to shorten an argument.
For example, \ww{No house cats have rabies} should not entail that
  \ww{cats have rabies}, or even that \ww{house cats have rabies}.
%Likewise, the sentence \ww{enemies give fake praise} should not
%  produce the triple \textit{(enemies; give; praise)}.

% Showing off
When careful attention to logical validity is necessary -- such as 
  textual entailment -- this approach captures even more subtle phenomena.
For example, whereas \ww{all rabbits eat fresh vegetables} yields
  \textit{(rabbits; eat; vegetables)}, the apparently similar sentence
  \ww{all young rabbits drink milk} does not yield
  \textit{(rabbits; drink; milk)}.

%% Conclude
%Combining these two insights, we create an open IE system which
%  captures long-range dependencies without requiring a large rule set
%  by learning to split long sentences into coherent shorter utterances
%  from which triples can be extracted,
%  and then exploits natural logic semantics to find the optimal granularity
%  for the arguments of those triples.
% Scores
We show that our new system performs well on a real world evaluation --
  the TAC KBP Slot Filling challenge \cite{key:2013surdeanu-kbpoverview}.
We outperform both an official submission based on
  open IE, and a baseline of replacing our extractor with Ollie,
  a state-of-the-art open IE systems.

%address two problems with modern open IE systems:
%  first, they either do not capture relations between mentions which
%  are not canonically connected (e.g., subject verb object triples) or
%  these long-range dependencies are captured using a large number of
%  sparse rules.
%We propose an alternate approach which learns a classifier over a
%  simple set of actions to split long sentences into coherent
%  independent clauses.
%
%Second, existing systems generally do a poor job of producing
%  canonical argument forms, either stripping relevant
%  information, or retaining irrelevant information in the argument.
%Both of these are harmful for downstream applications using the
%  output of the systems
%We address this problem by extracting more specific (i.e., shorter)
%  arguments only if they are logically warranted by the larger
%  context of the sentence.
%This allows us to extract the maximally specific arguments for each
%  extracted triple.


%% Challenge: granularity of arguments
%A second key challenge for open IE systems is choosing the right level of
%  granularity for the triples being expressed.
%To illustrate, example 2 in \reffig{teaser} would \naive ly produce
%  the extraction (Chancellor Merkel of Germany; visit; China);\footnote{
%    This is the actual output from OLLIE.
%  }
%  however, most downstream applications would strongly prefer the subject
%  of the triple to drop the prepositional phrase.
%For instance, when applying open IE to a conventional relation extraction
%  task, overly fine grained arguments like \ww{Chancellor Merkel of Germany}
%  must either be treated as atomic units (leading to recall errors),
%  use heuristic matching (potentially leading to harmful extractions
%  like \ww{Germany visited China}), or be further parsed by the application
%  (which is counterproductive to using open IE in the first place).
