\Section{clause}{Inter-Clause Open IE}

\begin{sidewaysfigure}
\begin{center}
\begin{tabular}{cc}
\hspace{-3ex}
  % Full tree
  \begin{dependency}[text only label, label style={above}]
    \begin{deptext}[column sep=-0.00cm]
      Born \& in \& a \& small \& town \&[-1ex] , \& she \& took \& the \&
        midnight \& train \& going \& anywhere \&[-1ex] . \\
    \end{deptext}
    \depedge[edge unit distance=1.25ex]{1}{5}{prep\_in}
    \depedge[edge unit distance=1.0ex]{5}{4}{amod}
    \depedge[edge unit distance=1.4ex]{5}{3}{det}
    \depedge[edge unit distance=1.0ex, edge style={darkred!60!black,thick,densely dotted}]{8}{1}{\textbf{\darkred{vmod}}}
    \depedge[edge unit distance=2.0ex, edge style={blue!60!black,thick}]{8}{7}{\darkblue{nsubj}}
    \depedge[edge unit distance=1.75ex]{8}{11}{dobj}
    \depedge[edge unit distance=1.0ex]{11}{10}{nn}
    \depedge[edge unit distance=1.4ex]{11}{9}{det}
    \depedge[edge unit distance=1.0ex]{11}{12}{vmod}
    \depedge[edge unit distance=1.0ex]{12}{13}{dobj}
  \end{dependency}
  &
  % Just Clause
  \begin{dependency}[text only label, label style={above}]
    \begin{deptext}[column sep=-0.05cm]
      she \& Born \& in \& a \& small \& town \\
    \end{deptext}
    \depedge[edge unit distance=1.25ex]{2}{6}{prep\_in}
    \depedge[edge unit distance=1.0ex]{6}{5}{amod}
    \depedge[edge unit distance=1.4ex]{6}{4}{det}
    \depedge[edge unit distance=2.0ex, edge style={blue!60!black,thick}]{2}{1}{\darkblue{nsubj}}
  \end{dependency}
  \\
  \\
  \textbf{\small{(input)}} & \textbf{\small{(extracted clause)}} \\
  % arrows
  \\
  $\downarrow$ & $\downarrow$ \\
  \\
  % entailments
  \begin{tabular}{ll}
    \ww{\small{she took the midnight train going anywhere}} & \ww{\small{she took the midnight train}} \\
    \ww{\small{Born in a small town, she took the midnight train}} & \ww{\small{\textbf{she took midnight train}}}  \\
    \ww{\small{Born in a town, she took the midnight train}} & $\dots$ \\
  \end{tabular} &
  \begin{tabular}{l}
    \ww{\small{\textbf{she Born in small town}}} \\
    \ww{\small{she Born in a town}} \\
    \ww{\small{\textbf{she Born in town}}} \\
  \end{tabular}
  \\
  % arrows 2
  \\
  $\downarrow$ & $\downarrow$ \\
  \\
  % extractions
  \begin{tabular}{ll}
    (she; took; midnight train) \\
  \end{tabular} &
  \begin{tabular}{l}
    (she; born in; small town) \\
    (she; born in; town)
  \end{tabular} 
\end{tabular}
\end{center}
\longcaption{An illustration of the Open IE clause shortening approach.}
{\label{fig:clausesplit}
An illustration of our approach.
From left to right, a sentence yields a number of independent clauses
  (e.g., \ww{she Born in a small town} -- see \refsec{clause}).
From top to bottom, each clause produces a set of entailed shorter
  utterances, and segments the ones which match an atomic pattern into an
  open IE relation triple (see \refsec{natlog}).
%Self-contained clauses are split off from the main sentence where appropriate
%  (\refsec{clause}).
%Then, each clause -- along with the original sentence -- is shortened until
%  the central relation in the clause becomes salient (\refsec{extraction}).
%Lastly, a set of simple pattern are applied to these clauses to transform them
%  into open IE triples.
%The fragments which produced the triples are bolded in the figure.
}
\end{sidewaysfigure}



% Introduce task
In the first stage of our method, we produce a set of self-contained clauses
  from a longer utterance.
Our objective is to produce a set of clauses which can stand on their own
  syntactically and semantically, and are entailed by the original 
  sentence (see \reffig{clausesplit}).
% Motivate more broadly
Note that this task is not specific to extracting open IE triples.
Conventional relation extractors, entailment systems, and other NLP
  applications may also benefit from such a system.

%% Example
%Taking the example in \reffig{clausesplit}, the sentence:
%\begin{quotation}
%\noindent\ww{Born in a small town, she took the midnight train going anywhere.}
%\end{quotation}
%should produce the additional clause:
%\begin{quotation}
%\noindent\ww{She [was] born in a small town.}
%\end{quotation}

% Formal description
We frame this task as a search problem over a dependency representation of a sentence.
At a given node in the parse tree, we classify each outgoing arc
  $e = p \xrightarrow{l} c$, from the governor $p$ to a dependent $c$ 
  with [collapsed] Stanford Dependency label $l$,
  into an action to perform on that arc \cite{key:stanford-dep}.\footnote{
    This system has been updated to operate on universal dependencies \cite{key:stanford-ud}.
    However, it was originally developed on Stanford Dependencies, and the experiments in this
      section are based on this earlier version.
  }
Once we have chosen an action to take on that arc, we can recurse on the
  dependent node.
%At each node of the parse tree, for each outgoing dependency arc going
%  from this parent node $p$ to a child node $c$ with label $l$:
%  $e = p \xrightarrow{l} c$, we define a set of actions we can perform.
We decompose the action into two parts: (1) the action to take on the outgoing
  edge $e$, and (2) the action to take on the
  dependent $c$.
For example, in our motivating example, we are considering the arc:
  $e = \ww{took} \xrightarrow{vmod} \ww{born}$.
In this case, the correct action is to
  (1) yield a new clause rooted at \ww{born},
  and (2) interpret the subject of \ww{born} as the subject
  of \ww{took}.

% Motivation
%We can view the actions in our clause splitter as ensuring that our
%  clauses are both \textit{short} and \textit{coherent}.
%\Naive ly splitting clauses on dependency edges shortens a sentence, 
%   but does not
%  always form a coherent constituent (e.g., \ww{born in a small town}).
%However, for a wide range of cases, this incoherence can be fixed by copying
%  part of the parent node. \todo{linguistics?}


We proceed to describe this action space in more detail, followed by an
  explanation of our training data, and finally our classifier.

%
% Action Space
%
\Subsection{actions}{Action Space}
% The actions
The three actions we can perform on a dependency edge are:

\begin{itemize}[leftmargin=2ex]
\item[] \textbf{Yield} \hspace{1ex}
  Yields a new clause on this dependency arc.
  A canonical case of this action is the arc 
    \ww{suggest} $\xrightarrow{ccomp}$ \ww{brush} in
    \ww{Dentists suggest that you should brush your teeth}, yielding
    \ww{you should brush your teeth}.

\item[] \textbf{Recurse} \hspace{1ex}
  Recurse on this dependency arc, but do not yield it as a new clause.
  For example, in the sentence \ww{faeries are dancing in the field where
  I lost my bike}, we must recurse through the intermediate constituent
  \ww{the field where I lost my bike} -- which itself is not relevant
  -- to get to the clause of interest: \ww{I lost my bike}.
%  This is often useful for long sentences, where a relevant clause is nested
%    within a constituent that should not itself be a clause.
%  For example, in the sentence \ww{Obama, our 44$^\textrm{th}$ president, is
%    the first African-American to hold the office}:
%    we would like to extract the clause 
%    \ww{Obama is our 44$^\textrm{th}$ president}, but not the intermediate
%    clause \ww{Obama, our 44$^\textrm{th}$ president}.

\item[] \textbf{Stop}  \hspace{1ex}
  Do not recurse on this arc, as the subtree under this arc is
    not entailed by the parent sentence.
  This is the case, for example, for most leaf nodes
    (\ww{furry cats are cute} should not entail the clause \ww{furry}),
    and is an important action for the efficiency of the algorithm.
%    but, also the case when the clause would not be entailed by the original
%    sentence (e.g., \ww{Dentists do not suggest that you should eat rocks}
%    should not entail that \ww{you should eat rocks}).
\end{itemize}

%\begin{tikzpicture}[
%  tlabel/.style={pos=0.4,right=-1pt,font=\footnotesize\color{red!70!black}}
%  ]
%  \tikzstyle{level 1}=[level distance=1.0cm, sibling distance=2.5cm]
%  \tikzstyle{level 2}=[level distance=1.0cm, sibling distance=2cm]
%  \tikzstyle{level 2}=[level distance=1.0cm, sibling distance=1cm]
%  \tikzstyle{vertex}=[]
%  \node[vertex] {*}
%    child { 
%      node[vertex] {A}
%      child {
%        node[vertex] {D}
%        edge from parent node[tlabel,left,pos=0.5] {Stop}
%      }
%      child {
%        node[vertex] {E}
%        edge from parent node[tlabel,right,pos=0.5] {Stop}
%      }
%      edge from parent node[tlabel,left,pos=0.3,shift={(-3 ex,0 ex)}] {Yield}
%    } child {
%      node[vertex] {B}
%      child {
%        node[vertex] {F}
%      } child {
%        node[vertex] {G}
%      }
%      edge from parent node[tlabel,right,pos=0.5] {Stop}
%    } child {
%     node[vertex] {C}
%     child {
%       node[vertex] {H}
%       child {
%         node[vertex] {I}
%         edge from parent node[tlabel,left,pos=0.5] {Stop}
%       }
%       child {
%         node[vertex] {J}
%         edge from parent node[tlabel,right,pos=0.5] {Stop}
%       }
%       edge from parent node[tlabel,right,pos=0.5] {Yield}
%     }
%     edge from parent node[tlabel,right,pos=0.3,shift={(3 ex,0 ex)}] {Recurse}
%    }
%  ;
%\end{tikzpicture}

% Introduce path
With these three actions, a search path through the tree becomes a sequence
  of \textbf{Recurse} and \textbf{Yield} actions, terminated by a \textbf{Stop}
  action (or leaf node).
For example, a search sequence 
  $A \xrightarrow{Recurse} B \xrightarrow{Yield} C \xrightarrow{Stop} D$
  would yield a clause rooted at $C$.
A sequence 
  $A \xrightarrow{Yield} B \xrightarrow{Yield} C \xrightarrow{Stop} D$
  would yield clauses rooted at both $B$ and $C$.
Finding all such sequences is in general exponential in the size of the tree.
In practice, during training we run breadth first search to collect the
  first \num{10000} sequences.
During inference we run uniform cost search until our classifier predictions
  fall below a given threshold.

% Introduce parent action
For the \textbf{Stop} action, we do not need to further specify an action
  to take on the parent node.
However, for both of the other actions, it is often the case that we would like
  to capture a controller in the higher clause.
%borrow some portion of the parent tree.
We define three such common actions:
%-- though this is not necessarily an exhaustive list:

\begin{itemize}[leftmargin=2ex]
\item[] \textbf{Subject Controller}  \hspace{1ex}
  If the arc we are considering is not already a subject arc,
    we can copy the subject of the parent node and attach it as a subject of the
    child node.
  This is the action taken in the example
    \ww{Born in a small town, she took the midnight train}.

\item[] \textbf{Object Controller}  \hspace{1ex}
  Analogous to the subject controller action above, but taking the object
    instead.
%  If the arc we are considering is not an object arc,
%    we copy the object of the parent node and attach it as an object of the
%    child node.
  This is the intended action for examples like
    \ww{I persuaded Fred to leave the room}, where Fred is taken as
    the object of \ww{persuade} in Stanford/Universal dependencies.

\item[] \textbf{Parent Subject} \hspace{1ex}
  If the arc we are taking is the only outgoing arc from a node, we take the
    parent node as the (passive) subject of the child.
  This is the action taken in the example
    \ww{Obama, our 44$^\textrm{th}$ president} to yield a clause with the
    semantics of \ww{Obama [is] our 44$^\textrm{th}$ president}.
\end{itemize}

% Additional actions
Although additional actions are easy to imagine, we found empirically that
  these cover a wide range of applicable cases.
We turn our attention to the training data for learning these actions.

%
% Training Data
%
\Subsection{distsup}{Training}
% Overview
We collect a noisy dataset to train our clause generation model.
We leverage the \textit{distant supervision} assumption for relation extraction,
  which creates a noisy corpus of sentences annotated with relation mentions
  (subject and object spans in the sentence with a known relation).
%  to create a noisy annotated corpus of sentences annotated with relations.
%Distant supervision is a method for training a supervised classifier using only
%  a large unlabeled corpus, and a knowledge base of known facts.
%We can then generate noisy sentence-based annotations, by assuming that every
%  sentence which contains two entities in our knowledge base 
Then, we take this annotation as itself distant supervision for a correct
  sequence of actions to take: any sequence which recovers the 
  known relation is correct.
%  whereas other action sequences are assumed to be
%  incorrect.

% Why KBP data?
We use a small subset of the KBP source documents for 
  2010 \cite{key:2010ji-kbpoverview}
  and 2013 \cite{key:2013surdeanu-kbpoverview}
  as our distantly supervised corpus.
To try to maximize the density of known relations in the training sentences,
  we take all sentences which have at least one known
  relation for every 10 tokens in the sentence,
  resulting in \num{43155} sentences.
In addition, we incorporate the \num{23725} manually annotated examples
  from \newcite{key:2014angeli-active}.
%  -- the relations in these sentences are generally less noisy.
%Although we train our mod on KBP data, we do not incorporate
%  anything particular about the KBP relation set.
%Our only assumption is that clauses which contain a KBP relation occur in
%  a similar distribution as clauses which contain arbitrary open-domain
%  relations.

% Distant supervision x 2
Once we are given a collection of labeled sentences, 
%we make make a second
%  assumption, akin to distant supervision for clause splitting.
  we assume that a sequence of actions which leads to a correct extraction of
  a known relation is a \textit{positive sequence}.
A correct extraction is any extraction we produce from our model
  (see \refsec{extraction}) which has the same arguments as the known
  relation.
For instance, if we know that Obama was born in Hawaii from the sentence
  \ww{Born in Hawaii, Obama $\dots$}, and an action sequence produces the triple
  \textit(Obama, born in, Hawaii), then we take that action sequence as
  a positive sequence.

% Negative sequences
Any sequence of actions which results in a clause which produces no relations
  is in turn considered a \textit{negative sequence}.
%Assuming perfect sentence level relation annotations, this is still only
%  distantly supervised training data.
%%This is, however, only distantly supervised training data.
%It's possible for an incorrect sequence of actions to produce the correct
%  extraction, or even more commonly for a correct sequence of actions to 
%  produce no extraction.
%
% Relation to Distant supervision #1
The third case to consider is a sequence of actions which produces a relation,
  but it is not one of the annotated relations.
This arises from the \textit{incomplete negatives} problem in distantly
  supervised relation extraction \cite{key:2013min-incomplete}: 
  since our knowledge base is not exhaustive,
  we cannot be sure if an extracted relation is incorrect or correct but
  previously unknown.
Although many of these unmatched relations are indeed incorrect, the dataset
  is sufficiently biased towards the STOP action that the occasional
  false negative hurts end-to-end performance.
Therefore, we simply discard such sequences.
%In the interest of erring on the side of higher recall -- particularly given
%  a dataset already heavily biased towards the \textit{stop} action --
%  we discard such sequences.

%Furthermore, we must still mitigate the noise in the distantly supervised
%  method for generating our annotated sentences.
%In particular, the problem of \textit{incomplete negatives}: there are sequences
%  of actions which produce a relation, but it is not the known relation in the
%  sentence \cite{key:2013min-incomplete}.
%This however does not inherently mean that it is an incorrect relation -- perhaps
%  it is a relation we don't know about in the knowledge base.

% Handling of examples
Given a set of noisy positive and negative \textit{sequences}, we construct 
  training data for our action classifier.
All but the last action in a positive sequence are added to the training set
  with the label \textbf{Recurse}; the last action is added with the label
  \textbf{Split}.
Only the last action in a negative sequence is added with the label \textbf{Stop}.
We partition the feature space of our dataset according to the action
  applied to the parent node.

%
% Classifier
%
\Subsection{clauseclassifier}{Inference}

\begin{table}
\begin{center}
\begin{tabular}{ll}
  \textbf{Feature Class} & \textbf{Feature Templates} \\
  \hline
  Edge taken          & $\{ l, \textrm{short\_name}(l) \}$ \\
  Last edge taken     & $\{ \textrm{incoming\_edge}(p) \}$ \\
  Neighbors of parent & $\{ \textrm{nbr}(p), (p, \textrm{nbr}(p)) \}$ \\
  Grandchild edges    & $\{ \textrm{out\_edge}(c), $ \\
                      & $~~ (e, \textrm{out\_edge}(c)) \}$ \\
  Grandchild count    & $\{ \textrm{count}\left( \textrm{nbr}(e_\textrm{child}) \right) $ \\
                      & $~~ \left(e, \textrm{count}\left( \textrm{nbr}(e_\textrm{child}) \right) \right) \}$ \\
  Has subject/object  & $\forall_{e \in \{e, e_\textrm{child}\}} \forall_{l \in \{\textit{subj}, \textit{obj}\}} $ \\
                      & $~~ \1(l \in \textrm{nbr}(e)) $ \\
  POS tag signature   & $\{ \textrm{pos}(p), \textrm{pos}(c), $ \\
                      & $~~ \left( \textrm{pos}(p), \textrm{pos}(c) \right) \}$ \\
  Features at root    & $\{ \1(p = root), \textrm{POS}(p) \}$
\end{tabular}
\end{center}
\longcaption{Features for the Open IE clause splitter model}
{\label{tab:features}
Features for the clause splitter model, deciding to split on the arc
  $e = p \xrightarrow{l} c$.
The feature class is a high level description of features; the feature
  templates are the particular templates used.
For instance, the POS signature contains the tag of the parent, the tag of
  the child, and both tags joined in a single feature.
Note that all features are joined with the action to be taken on the parent.
}
\end{table}

% Classifier
We train a multinomial logistic regression classifier on our noisy
  training data, using the features in \reftab{features}.
The most salient features are the label of the edge being taken, the
  incoming edge to the parent of the edge being taken, neighboring edges
  for both the parent and child of the edge, and the part of speech tag of
  the endpoints of the edge.
The dataset is weighted to give $3\times$ weight to examples in the \textbf{Recurse}
  class, as precision errors in this class are relatively harmless for accuracy,
  while recall errors are directly harmful to recall.

%% Introduce performance
%We report crude performance numbers for our classifier, with the
%  caveat that the performance ceiling of the classifier is bounded by the
%  noise in the dataset itself.
%%Note that the performance ceiling for the task is far below 100\%.
%%Precision errors are inevitable due to the incompleteness of our
%%  knowledge base (we find relations which are correct but not yet known);
%%  recall errors are in turn often due to the noise of distant supervision.
%% Statistics
%We compute precision and recall by averaging
%  over 5-fold cross validation on a dataset of
%  \num{1159395} datums -- of which \num{56962} have label \textbf{Split} and
%  \num{65318} have label \textbf{Recurse}.
%The \textbf{Stop} action is taken to be the negative class.
%% Performance
%For the \textbf{Split} class, we achieve a precision of 79.2, with a recall
%  of 65.7 (71.8 F$_1$)
%For the \textbf{Recurse} class, we achieve a precision of 41.1, with a recall
%  of 64.9 (50.3 F$_1$)

% Inference
Inference now reduces to a search problem.
Beginning at the root of the tree, we consider every outgoing edge.
  For every possible action to be performed on the parent (i.e., clone subject,
  clone root, no action), we apply our trained classifier to determine
  whether we 
  (1) split the edge off as a clause, and recurse;
  (2) do not split the edge, and recurse; or 
  (3) do not recurse.
In the first two cases, we recurse on the child of the arc, and continue until
  either all arcs have been exhausted, or all remaining candidate arcs
  have been marked as not recursable.

% Score
We will use the scores from this classifier to inform the score assigned to
  our generated open IE extractions (\refsec{extraction}).
The score of a clause is the product of the scores of actions taken to reach
  the clause.
The score of an extraction will be this score multiplied by the score
  of the extraction given the clause.
