A recurring theme throughout this thesis will be the use of natural
  logics for natural language tasks.
%
% Motivate Logic
%
To begin, we should briefly motivate the use of any logical formalism at all
  for natural language. %I'll begin by motivating the use of any logic at all for natural language.
A wide range of successful apporaches in NLP neither use nor need logical reasoning 
  to perform their task:
  statistical parsing, part-of-speech tagging, document classification, etc. are all
  fairly agnostic to any logical phenomena in the documents they are operating over.
However, as the field of NLP moves away from syntax and shallow classification tasks and
  increasingly towards natural language \textit{understanding}, we are faced with a
  necessity to understand meaning of sentences, and understand the implications that can
  be drawn from them.

Taking some simple examples across a few tasks: in relation extraction, it would be
  incorrect to assume that Obama was born in Kenya from a sentence \ww{Obama was not
  born in Kenya}.
For sentiment analysis, the sentence \ww{The movie's box office failure should not be taken as
  indicative of its quality} should get positive sentiment.
In question answering, returning \ww{Harrison Ford} for a query asking for all the female
  stars of Star Wars would be incorrect.

In all of these cases, a system must either explicitly or implicitly learn how to handle these
  sorts of logical subtleties.
Implicitly, a neural network or sufficiently powerful statistical classifier could learn how
  to treat these queries, at the expense of sufficiently large amount of training data
  (in the extreme, one can always simply memorizing the logic).
Explicitly, a first order theorem prover or Markov Logic network could capture the logical
  content of a sentence, at the expense of a drop in expresivity and computational
  efficiency.
In this chapter we'll motivate natural logics as an optimal midway between offloading this sort
  of logical reasoning entirely to statistical / neural methods, and enforcing a rigid
  logical framework on the problem.


%
% Introduce Natural Logics
%
Broadly, the aim of natural logics are to capture a subset of valid logical
  inferences by appealing directly to the structure of language,
  as opposed to running deduction in an abstract logical language (e.g., well-formed first-order formulae
  composed predicates, variables, connectives, and quantifiers).
That is to say, a natural logic is primarily a \textit{proof theory}, where
  the deductive system operates over natural language syntax.
To illustrate with an example, we can consider a simple premise:

\begin{displayquote}\ww{
\ww{The cat ate a mouse.}
}\end{displayquote}

From this, we can run the following logical derivation, using the syntax of natural
  language as the proof language:

\[
\begin{nd}
\hypo {1} {\ww{The cat ate a mouse.}}
\have {2} {\ww{The cat ate a rodent.}}
\have {3} {\ww{The cat ate an animal.}}
\have {4} {\ww{The feline ate an animal.}}
\have {5} {\ww{The carnivore ate an animal.}}
\have {6} {\lnot ~~ \ww{No carnivore ate an animal.}}
\end{nd}
\]

That is to say, if the cat ate a mouse, it is false that no carnivore ate an
  animal, and this is by virtue of the ``proof'' presented above.
Although it's not unnatural to use a line of reasoning like the one above to justify
  a conclusion informally, it may be strange
   the perspective of propositional and
  first-order logics to treat this as a formal proof.
Nonetheless, it is not an altogether unfamiliar sort of reasoning.
Natural logics can trace their origins to the syllogistic reasoning of Aristotle.
For instance, the proof above is similar to a simple syllogism:

%
% Syllogisms
%

\[
\begin{nd}
\hypo {1} {\ww{All cats eat mice.}}
\hypo {2} {\ww{All cats are carnivores.}}
\have {3} {\ww{All carnivores eat mice.}}
\end{nd}
\]

We can furthermore chain these syllogisms in much the same way you would
  chain a first-order proof.
To introduce a motivating example, an Athenian philosopher might retort with the following
  if a Persian invader claims that all heroes are Persians:


\begin{displayquote}\ww{
  Clearly you are wrong. You see, all Gods live on Mount Olympus.
  Some heroes are Gods. And, of course, no one who lives on Mount Olympus
  is Persian.
}\end{displayquote}


\begin{figure}
\[
\begin{nd}
\hypo {1} {\forall x~~\textrm{God}(x) \supset \textrm{LivesOnOlympus}(x)}
\hypo {2} {\exists x~~\textrm{Hero}(x) \land \textrm{God}(x)}
\hypo {3} {\lnot \exists x~~\textrm{LivesOnOlympus}(x) \land \textrm{Persian}(x)}

\open
  \hypo {4} {\forall x~~\textrm{Hero(x)} \supset \textrm{Persian}(x)}
  \open[a]
    \hypo {5}  {\textrm{Hero}(a) \land \textrm{God}(a)}                          \Ee{2}
    \have {6}  {\textrm{Hero}(a)}                                                \ae{5}
    \have {7}  {\textrm{Hero(a)} \supset \textrm{Persian}(a)}                    \Ae{4}
    \have {8}  {\textrm{Persian}(a)}                                             \ie{6,7}
    \have {9}  {\textrm{God}(a)}                                                 \ae{5}
    \have {10} {\textrm{God}(a) \supset \textrm{LivesOnOlympus}(a)}              \Ae{1}
    \have {11} {\textrm{LivesOnOlympus}(a)}                                      \ie{9,10}
    \have {12} {\textrm{LivesOnOlympus}(a) \land \textrm{Persian}(a)}            \ai{8,11} 
    \have {13} {\exists x~~\textrm{LivesOnOlympus}(x) \land \textrm{Persian}(x)} \Ei{12} 
  \close
  \have {14} {\exists x~~\textrm{LivesOnOlympus}(x) \land \textrm{Persian}(x)}   \r{12} 
  \have {15} {\bot}                                                              \by{$\bot$ I}{3,14}
\close
\have {16} {\lnot~~\forall x~~\textrm{Hero(x)} \supset \textrm{Persian}(x) }     \by{$\lnot$ I}{4--15}
\end{nd}
\]
\caption{\label{fig:natlog-fol-syllogism}
  A Fitch-style first order logic proof refuting \ww{``all heroes are Persian''} given the premises
    \ww{``All Gods live on Mount Olympus,''} \ww{``Some heroes are Gods,''} and \ww{``No one who lives
    on Mount Olympus is Persian.''}.
  The proof follows a proof by contradiction (lines 4--15), hinging on showing that the hero-God
    hypothesized in premise 2, and instantiated as $a$ on line 5, would have to be a Persian who lives
    on Olympus.
  This would contradict premise 3.
}
\end{figure}


Sadly our poor philosopher will have likely perished by this point -- lest a Greek hero
  saves him, in which case then there would remain neither a need nor an audience for
  his argument.
But but I can still convince you, the reader, that this was a valid retort by constructing 
  a first-order proof, given in \reffig{natlog-fol-syllogism}.
The proof follows a proof by contradiction, showing that the hypothesized hero-God (\ww{``some
  heroes are Gods''}) would have to both live on Olympus and be Persian -- a contradiction with
  our premise: \ww{``no one who lives on Mount Olympus is Persian.''}

To contrast with the first order logic proof, there also exists a natural logic proof of our
  contradiction, based entirely on Aristotilean syllogisms.
This proof makes use of two syllogistic patterns chained together, and one of the axiomatic
  negations:

\[
\begin{nd}
\hypo {1} {\ww{All Gods live on Mount Olympus}}
\hypo {2} {\ww{Some heroes are Gods}}
\hypo {3} {\ww{Nobody who lives on Mount Olympus is Persian}}
\have {4} {\ww{Some heroes live on Mount Olympus}}  \by{AII (Darii)}{1,2}
\have {5} {\ww{Some heroes are not Persian}}        \by{EIO (Ferio)}{4,3}
\have {6} {\lnot~~\ww{All heroes are Persian}}      \by{SaP $\bot$ SoP}{5}
\end{nd}
\]

%
% Motivation
%
This example -- and in particular the contrast between the two proof approaches above --
  provides an excellent context for motivating why the enterprise of natural logics is worthwhile,
  and why research in the area can have a large impact on natural language processing.
I'll highlight three concrete motivations in more detail: (1) natural logics are easy to parse into,
  (2) this parsing is, in a sense, ``lossless,'' and (3) the proofs have the potential to be much
  more efficient than first order approaches.

% Easy to parse into
\paragraph{Natural logics are easy to parse into.}
When performing inference in propositional or first order logic, the premises are no longer in
  natural language, but rather have been parsed into a special logical form.
%The premises in the first order proof in \reffig{natlog-fol-syllogism} are necessarily no
%  longer in natural language, but parsed into a special logical form.
Ignoring the aesthetics of the logical form itself, this mapping is by no means trivial;
  in fact, an entire subfield of NLP -- semantic parsing -- focuses exclusively on this problem
  \cite{key:2008kate-semantics,key:2005zettlemoyer-semantics,key:2011liang-semantics,key:2013berant-sempre}.
Even in our simple example, an intuitively simple utterance like \ww{``some heroes are Gods''}
  parses into a premise which reads  most naturally as \ww{``there exists
  something which is a hero and a God.''}
Even more subtly, consider the difference in form for the following two translations:


\vspace{1em}
\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Sentence} & \textbf{Logical Form} \\
\midrule
\ww{Everyone on Mount Olympus is Persian} 
  & $ \forall x~~\textrm{LivesOnOlympus}(x) \supset \textrm{Persian}(x) $ \\
\ww{No one on Mount Olympus is Persian} 
  & $ \lnot \exists x~~\textrm{LivesOnOlympus}(x) \land \textrm{Persian}(x)$ \\
\bottomrule
\end{tabular}
\end{center}
\vspace{1em}

Despite the lexical forms being nearly identical, the logical form has an entirely different
  structure, changing not only the quantifier but also the connective ($\supset$ versus $\land$).
By contrast, ``parsing to a logical form'' in syllogistic logic is nonexistent, and
  in the more sophisticated natural logics later in this chapter generally reduces to a
  shallow syntactic parse of the sentence.

%  Lossless parsing
\paragraph{Natural logic parsing is ``lossless.''}
In general, natural language has more semantic (not to mention pragmatic)
  content than the logical propositions we extract from it.
This is evident in the fact that it's necessary to retrain semantic parsers for new tasks, and underlies
  the research agenda of defining general semantic representations -- e.g., the Abstract Meaning
  Representation \cite{key:2013banarescu-amr}.
Despite being picked to be as concise as possible, even our motivating example
  has hints of this.
The derivation in \reffig{natlog-fol-syllogism} has defined the predicates necessary
  for our particular derivation (LivesOnOlympus, God, Hero, Persian); but, these are not
  the only things that could be extracted from the sentences.
Semantically, we could extract that Gods are alive.
Pragmatically, we should extract that there exist heroes.

To contrast, by operating over natural language directly, the limitations on what
  natural logics can infer are entirely due to the limitations of the inference
  mechanism, rather than limitations in what information was successfully extracted
  from the sentence.

\paragraph{Natural logic proof are efficient.}
Annecdotally, it should be clear that the proof in \reffig{natlog-fol-syllogism} is
  longer and more nuanced than the corresponding syllogistic proof.
In fact, to a first approximation, one could make a syllogistic theorem prover with
  nothing more than regular expressions and a small search algorithm.
Of course, more expressive natural logics have more difficult search problems;
  but, nonetheless, they remain in the same spirit of searching over lexical mutations
  rather than applying symbolic rule sets.

Contrasting with approaches like Markov Logic Networks makes this difference especially salient
  \cite{key:2006richardson-mln}.
Formulating our motivating example as a Markov Logic Network requires outright instantiating
  the denotations of our predicates on a closed world.
That is, we must first enumerate all the heroes, Gods, and residents of Olympus.
Thereafter, the task of grounding our formulae on this world and running heavyweight
  statistical inference algorithm the resulting Markov Network.
Making this grounding and inference process tractable is in itself an active area of
  research \cite{key:2011niu-tuffy,key:2014zhang-dimmwitted}.
By contrast, natural logics are almost by definition a \textit{proof-theoretic} logic:
  quantifiers can be reasoned over efficiently using only lexical mutation based proofs.
Although they have a model-theoretic interpretation, actually simulating this model
  is unecessary and would be impracital.


% What's missing?
Of course, despite these advantages it would be unreasonable to advocate for syllogistic
  reasoning as a practical formalism for modern AI applications.
Syllogisms don't allow for compositionality, and are generally restrictive in
  the range of inferences they warrant.
This thesis instead adopts variants of \textit{monotonicity calculus}
  \cite{key:1986benthem-natlog,key:1991valencia-natlog}
  -- a more general-purpose logic which handles a wide range of common phenomena in human
  language, while nonetheless still operating over the syntax of the language itself.


%
% Outline
%
The remainder of chapter will review denotational semantics (\refsec{natlog-denotations})
  as a prelude to introducing the monotonicity calculus of \newcite{key:1991valencia-natlog},
  described in detail in \refsec{natlog-mono}.
From here on, \textit{natural logic} will be used to refer to the monotonicity calculus
  described in \newcite{key:1991valencia-natlog}.
These sections are intended to give a \textit{model-theoretic} interpretation of the logic
  that can then be used to justfiy the potentially ad-hoc seeming proof theory described
  here.
We motivate further research into related formalisms in \refsec{natlog-propositional}.
