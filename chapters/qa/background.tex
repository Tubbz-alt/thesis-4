\Section{bg}{Background}
We briefly review natural logic and the existing NaturalLI system.
Much of this paper will extend this system, with additional inferences
  (\refsec{naturalli}) and a soft lexical classifier (\refsec{softsignal}).

\Subsection{bg-natlog}{Natural Logic}
% Bag of citations
Natural logic is a formal proof theory that 
  aims to capture a subset of logical
  inferences by appealing directly to the structure of language,
  without needing either an abstract logical language 
  (e.g., Markov Logic Networks; \newcite{key:2006richardson-mln})
  or denotations (e.g., semantic parsing; \newcite{key:2015liang-denotations}).
%Like \newcite{key:2014angeli-naturalli}, we use the logic
We use the logic
  introduced by the NatLog system (MacCartney and Manning, 2007; 2008; 2009),
  \nocite{key:2007maccartney-natlog}
  \nocite{key:2008maccartney-natlog}
  \nocite{key:2009maccartney-natlog} % no comma here -- they're nocites
  which was in turn
  based on earlier theoretical work on Monotonicity Calculus
  \cite{key:1986benthem-natlog,key:1991valencia-natlog}.
We adopt the precise semantics of \newcite{key:2014icard-natlog};
  we refer the reader to this paper for a more thorough introduction to
  the formalism.

% Give a teaser
At a high level, natural logic proofs operate by mutating spans of text
  to ensure that the mutated sentence follows from the
  original -- each step is much like a syllogistic inference.
Each mutation in the proof follows three steps:

\begin{enumerate}
\setlength\itemsep{-0.25em}
\item An atomic lexical relation is induced by either inserting, deleting
      or mutating a span in the sentence. 
      For example, in \reffig{naturalli},
      mutating \w{The} to \w{No} induces the \negate\ relation;
      mutating \w{cat} to \w{carnivore} induces the \forward\ relation.
      The relations \equivalent\ and \forward\ are variants of  entailment;
      \negate\ and \alternate\ are variants of negation.

\item This lexical relation between words is projected up to yield a relation between
      sentences, based on the polarity of the token (defined below).
      For instance, \w{The cat eats animals} \forward\ \w{some carnivores eat animals}.
      We explain this in more detail below.

\item These sentence level relations are \textit{joined} together to produce a
      relation between a premise, and a hypothesis multiple mutations away.
      For example in \reffig{naturalli}, if we join 
      \forward, \equivalent, \forward, and \negate, we
      get negation (\alternate).
\end{enumerate}

%Some example relations are given below, corresponding to the example inference
%  in \reffig{naturalli}:
%
%\vspace{1.0em}
%\begin{center}
%\begin{tabular}{rcl}
%\w{No $x$ $y$} & \negate     & \w{The $x$ $y$} \\
%\w{cat}        & \forward    & \w{carnivore} \\
%\w{animal}     & \equivalent & \w{a animal} \\
%\w{animal}     & \reverse    & \w{mouse} \\
%\end{tabular}
%\end{center}
%\vspace{1.0em}
%
%% Punt on things
%We refer the reader to \newcite{key:2014icard-natlog} for a
%  more comprehensive introduction to the atomic relations (1), and to the
%  join table between these relations (3).

The notion of \textit{projecting} a relation from a lexical item to
  a sentence is important to understand.\footnote{
    For clarity we describe a simplified semantics here; 
    NaturalLI implements the semantics described in 
    \newcite{key:2014icard-natlog}.
  }
To illustrate, \textit{cat} \forward\ \textit{animal},
  and \textit{some cat meows} \forward\ \textit{some animal meows}
  (recall, \forward\ denotes entailment),
  but
  \textit{no cat barks} \nforward\ \textit{no animal barks}.
Despite differing by the same \textit{lexical} relation,
  the \textit{sentence-level} relation is different in the two cases.

% Monotonicity
We appeal to two important concepts: \textit{monotonicity} -- a
  property of arguments to natural language operators;
  and \textit{polarity} -- a property of tokens.
From the example above, \w{some} is [upward] monotone in its first
  argument (i.e., \w{cat} or \w{animal}), 
  and \w{no} is antitone (downward monotone) in its first argument.
This means that the first argument to \w{some} is allowed to mutate up the
  hypernymy tree, whereas the first argument to \w{no} is allowed to mutate
  down the hypernymy tree.

% Polarity
\textit{Polarity} is
  a property of tokens in a sentence determined by the
  operators acting on it.
All lexical items have \textit{upward} polarity by default;
  monotone operators -- like \w{some}, \w{several}, or \w{a few} -- preserve polarity.
Antitone operators -- like \w{no}, \w{not}, and \w{all} (in its first argument) -- 
  reverse polarity.
For example, \w{mice} in \w{\textbf{no} cats eat mice} has downward 
  polarity, whereas \w{mice} in \w{\textbf{no} cats do\textbf{n't} eat mice}
  has upward polarity
  (it is in the scope of two downward monotone operators).

%The polarity of a token defines how a relation induced by a lexical mutation
%  projects up a sentence, and therefore completely determines the valid mutations
%  on that token.

%
% NaturalLI
%
\Subsection{bg-natlog}{NaturalLI}

% Teaser search figure
\begin{figure}[t]
\begin{center}
  \resizebox{0.48\textwidth}{!}{\teaserSearch} \\
\end{center}
\caption{
  An illustration of NaturalLI searching for a candidate premise
  to support the hypothesis at the root of the tree.
  We are searching from a hypothesis
  \w{no carnivores eat animals}, and find a contradicting
  premise \w{the cat ate a mouse}.
  The edge labels denote Natural Logic inference steps.
  \label{fig:naturalli}
}
\end{figure}

% Introduce NaturalLI
We build our extensions within the framework of NaturalLI, introduced
  by \newcite{key:2014angeli-naturalli}.
NaturalLI casts inference as a search problem: given a hypothesis and an arbitrarily
  large knowledge of text, it searches through the space of
  lexical mutations (e.g., \w{cat} $\rightarrow$ \w{carnivore}), 
  with associated costs, until a premise is found.
%This allows it to capture both strict natural logic inferences (e.g., over hypernymy
%  or operator mutations), as well as likely correct inferences with an associated
%  cost (e.g., nearest neighbors in vector space).

An example search using NaturalLI is given in \reffig{naturalli}.
The relations along the edges denote relations between the associated sentences
  -- i.e., the projected lexical relations from \refsec{bg-natlog}.
Importantly, and in contrast with traditional entailment systems, NaturalLI
  searches over an arbitrarily large knowledge base of textual premises rather
  than a single premise/hypothesis pair.
%Most importantly for a question answering task, however, NaturalLI allows for an
%  arbitrarily large knowledge base to search for answers in, contrasted with traditional
%  entailment systems which classify entailment between one (or a few) premises and a
%  hypothesis.

% Explain example
%In practice, NaturalLI generally searches around \num{100000} candidate premises
%  per second, on knowledge bases ranging from 1M premises (the \textsc{Scitest}
%  corpus used in this work) to 300M premises (the dataset used in the original paper).
